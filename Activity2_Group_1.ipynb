{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_T6hVxOJCVA"
   },
   "source": [
    "# CpE Elective 3 Laboratory (LBYCPC4) <br/>\n",
    "#### Computer Vision and Deep Learning Elective Track\n",
    "<br/>\n",
    "<hr style=\"border:5px solid gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYngdfKRVFMy"
   },
   "source": [
    "## Activity 2: Variational Autoencoders and Generative Adversarial Networks\n",
    "\n",
    "### Introduction\n",
    "&emsp;&emsp;&emsp;A Variational Autoencoder (VAE) and a Generative Adversarial Network (GAN) are both types of generative models, which are neural networks that can learn to produce new data that's similar to the data they were trained on. While they both serve this purpose, they achieve it using different architectures and training methodologies. In this activity, you will explore variational autoencoders and generative adversarial networks as generative models that has two subnetworks within itself.\n",
    "\n",
    "Variational autoencoders (VAE) are probabilistic generative models that use a combination of encoder and decoder networks with Bayesian inference. They learn a probabilistic mapping from a data distribution to a continuous, lower-dimensional space, known as the latent space.\n",
    "\n",
    "Generative adversarial networks (GAN) consist of two neural networks: a generator and a discriminator. The generator creates new data points, while the discriminator tries to distinguish between real and generated data. The two components are trained in an adversarial manner, improving each other over time.\n",
    "\n",
    "### Objectives\n",
    "- Understand the working principle and architecture of variational autoencoders and generative adversarial networks\n",
    "- Build and train a variational autoencoder using deep learning framework\n",
    "- Build and train a generative adversarial network using deep learning framework\n",
    "- Visualize the output of the generative networks\n",
    "- Assess the performance of the implemented generative networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaBV0GFgArTj"
   },
   "source": [
    "### Activity\n",
    "\n",
    "Execute the code cells below to install the required Python packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sdwvCCWEBBKe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 15:27:26.797379: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-28 15:27:27.407488: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-28 15:27:30.070325: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZPOtcykVQkx"
   },
   "source": [
    "**A. Variational Autoencoders**\n",
    "\n",
    "1. Acquire and preprocess the [EMNIST Dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.EMNIST.html). Image pixel values should be normalized and oriented properly. Display at least thirty (30) training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "hPujKGu4XQRJ",
    "outputId": "9d2ee46f-1a47-41dd-e5f0-de7e6db8f8b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Len: 112800\n",
      "Test Len: 18800\n",
      "X_train: (112800, 28, 28)\n",
      "y_train: (112800,)\n",
      "X_test: (18800, 28, 28)\n",
      "y_test: (18800,)\n",
      "Min Pixel 0\n",
      "Max Pixel 255\n",
      "Min Pixel 0.0\n",
      "Max Pixel 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAGZCAYAAADILvv1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAleZJREFUeJzt3Xe8VOW1+P91VBCBQ+8dpEgRbFdBpamELqJGjahYkmiCsVxbjAUbXsV4lWtBk5soGmsURFBARA4gAmoQpIlU6QKCNGkKvz/yu893rQUznjL7zJk9n/frlddrbdc5ZzbzzC6z86z15Bw8ePCgAAAAAAAAACl2RLp3AAAAAAAAAPHEgycAAAAAAABEggdPAAAAAAAAiAQPngAAAAAAABAJHjwBAAAAAAAgEjx4AgAAAAAAQCR48AQAAAAAAIBIHJWfHzpw4ICsW7dOcnNzJScnJ+p9QhIHDx6UHTt2SJ06deSII4r23JBxLTkY13hK5biKMLYlBeMaT4xrfHGNjSeO2XhiXOOJcY2ngoxrvh48rVu3TurXr5+SnUNqrF69WurVq1ekv8G4ljyMazylYlxFGNuShnGNJ8Y1vrjGxhPHbDwxrvHEuMZTfsY1X48bc3NzU7JDSJ1UjAnjWvIwrvGUqjFhbEsWxjWeGNf44hobTxyz8cS4xhPjGk/5GY98PXhi+lrJk4oxYVxLHsY1nlI1JoxtycK4xhPjGl9cY+OJYzaeGNd4YlzjKT/jQXNxAAAAAAAARCJfPZ4AAEgH36jw6KOPDnHNmjVNbvXq1SH+6aefot0xHML/v10HDx7MyNdAwRx55JEh9lPtt2/fHuIDBw4U2z7h3446KjW3+X7sGEt41atXD/GZZ55pcvq88MYbb5jc3r17o90xACUGM54AAAAAAAAQCR48AQAAAAAAIBKU2iEj+OnipUqVCrEvt/E/u23bthBv2bLF5CjHSS7ZNP0ff/yxGPcE2aRJkyYhfuSRR0zulFNOCbGe2i8iMn78+BA/+OCDJjdv3rwQU55VeFWqVDHbl156aYgbNWpkck8//bTZXrlyZaFe85hjjgmx/zysX78+xEOHDjU5yoGKR+/evUPcr18/k3v11VdDPGnSpGLbp2ymj8P+/fubXIUKFfL9d/TxM3/+fJObO3duiPUxKGJLpzgG40WXuj/33HMmd+GFF4Z48eLFJvfb3/42xHwmgILRLSeStRvIhGOLGU8AAAAAAACIBA+eAAAAAAAAEAkePAEAAAAAACAS9HhCiaFrx0VEWrRoEeKzzz7b5OrWrRviLl26mFy5cuXMtu7t8o9//MPkJkyYEGKWdD20R4vvD6GNGjUqxIXt3YLs5GvUL7nkErP9xBNPhLhGjRomt2vXrhD/5S9/MbmdO3eG+JVXXjG5P/3pTyEeM2ZMAfcY/6dx48Zm+4Ybbghx6dKlTU733BLJ/3lC93QSETn//PNDfPXVV5vcggULQvzYY4/l6++jaHzvv0cffTTE+rotItKrV68Qt2/f3uS++eabCPYu+5QvX95sDxw4MMS33HKLyZUpUybff1f3Dvn+++9NbvXq1SGePHmyyU2dOjXEY8eOTfg3kXn08XzZZZeZ3K9//esQ++svPUGBxPw9sb+Xys3NDXHZsmVNbseOHSHW98AiIvv370/VLqYMM54AAAAAAAAQCR48AQAAAAAAIBKU2iGtdHmdLqcQEbn99ttD3LRpU5M78sgjDxuLHDplUf+u/zu69OPrr782uT179iTb9djQZRMXXHCByd177735+htPPfWU2WZaNTxdPuWP9b/+9a9mWx/Db7/9tsk9+OCDIdZltCK2jGP27Nkm17JlyxBTalcwutzxtddeM7kmTZqE+KGHHjK5Tz75JN+voZcLfuaZZ0zu4osvDvGMGTNM7r/+679C/NNPP+X79VB4pUqVMtu+TF779NNPQ7xhw4bI9inuqlatarYbNmwY4nPPPdfkrrjiihD71gN6ue2tW7eanN/W/Ji3atUqxG3btjW5fv36hdgfr5s3b074Gih59HlZxJYz+1LZl156KcTZWlKp712y9T1AwVWvXt1sn3766Wa7d+/eId6+fbvJ6XJmf9+7bdu2VO1iyjDjCQAAAAAAAJHgwRMAAAAAAAAiwYMnAAAAAAAARIIeTyhWvh9TtWrVQnzVVVeZXK1atUK8ePFik8vLywuxXkpSRKRChQpmu3///iH2Sz3rXjOjRo0yuS+//DLEca7VrlKlSog7dOhgcrovj++f4t9nQPO9IXTfnksuucTkfF82vSzz66+/bnL57ePz7rvvmm2/5DgS80uu9+zZM8SNGzc2ud27d4d45MiRJvfDDz/k+zX1EsGdOnVKuD/jxo0zuWnTpuX7NZAazZs3N9v6Wu3pnhN79+6NbJ/iSJ+zfvvb35qcvl+qW7duwr/h++DNnDkzxL4H26xZs8y2PtdWqlTJ5G688cYQX3jhhSbXqFGjEDdo0MDk6PGUWXz/Nj2eQ4YMMbk43ycn4q+V7du3D/GmTZtMbsGCBcWyT8gM+r63Tp06JnfiiSea7ZNPPjnEEydONDndmy8TrrHMeAIAAAAAAEAkePAEAAAAAACASEReaufLLfy2FvUS7AXZlyiwxPyhZTJ62vWwYcNMTi/fq8veRETWr18f4v3795ucLg8TscsH33DDDSZ32WWXhVgvTywicttttx12PzOd/9yfccYZIfaldnoq/saNG01OvycFmWKtyy1zc3NNTo+dXwb6+++/N9vfffddoV4fxaNJkyZmu0+fPiH209Pffvtts63L6/JbWuf5z4QvyYWlS930uU9E5NZbbw3xnj17TO7Pf/5ziH1Zj6enlvsl2J9++ukQ+/IcXTapSzZFRPbt25f0NVF0Rx1lbxX/+te/mm19PPvPhy9hR2L6GBSxZaW61ELELqn95ptvmtyTTz4Z4q+//trk9PgU5LpZunRps/3BBx+EuFu3biZXo0aNEA8aNMjkbrnllhD7azoOpd9LEXsu9CXrK1euTPnrt2zZ0mzrsp5nn3025a+Xafxn/5FHHgnxhg0bTO4Xv/hFiAt7X4P40OdUX1rnt6tXrx5i/d1HxN7bZsJzBmY8AQAAAAAAIBI8eAIAAAAAAEAkePAEAAAAAACASKSkx5NfClsvv+nrX9u1axfiXbt2mZxeItDXxibrsaNfv2rVqiane0X4Hj5t2rQx26no+aT7CYmI7Ny5M8R+CcTFixeH2PcpyhZ66Ue/TLbm39dk/OdK9ym6/PLLTU5/Pjp27JgwF6ceT3553NNPPz3EfsnkqVOnhnj48OEmp8fO16vrY1LXJovY91n3/RERqV27doh9nxe9DLSIyMMPPxziFStWmBz18+mhe73885//NLlq1aqF2I+X7ynE+BW//v37h/iKK64wOd17RvcYERF5+eWXQ/xzPWP0NVj3jRIROeWUU0Lse5W8+OKLIfY9hBANfQ5v1KiRyfltfX1+//33TY4lxJPT951nn322yem+TrqvpYjIAw88EGJ/76SXcU9V/0N/36B7Nfox1ud6f2/Lub1g/LXx1FNPDfEFF1xgco8//nhKXlN/Jv/0pz+Z3JYtW0Ks+z1lq5NOOslsN2vWLMS+N57/rlwY/nuqPy579OgRYn8PPWbMmBAvX768yPuCgvGfB/195w9/+IPJ6c+RiD3uPvroI5Nbt25diOnxBAAAAAAAgKzFgycAAAAAAABEIiWldn6Z1Vq1aoVYT98XETnttNNC/MMPP5icLtP49NNPTS4vLy/h6+uphn45+E6dOoW4cePGJte6dWuzHXWpnV9CXJcT6ml0h/s72aAg/2Y95npat8ih76X+LM2YMcPkdKmXnvYoItK5c+cQf/nllyaXCdMZE6lZs6bZ7tq1a4j9VFA9LV5P3xex74GfQqyXSr/ppptMrmfPniGuUqWKyemyAD2VX+TQUtnKlSuH+IUXXjC5CRMmhFiXBCJaF198cYiPP/54k9MlsH4Z5iiWgUZy5cuXN9u69M2XUuljSJe9iYh88803CV/DnxcGDhwY4vPOO8/k9LlnyJAhJqePZxQPfb/06quvmpy/5uprwyuvvGJylFYld8wxx4RY36+K2OuhLmkVsaXM/l46CnrJbhGR1157LcT+89C+ffsQ6xJaEdsOw//NTHfjjTea7dGjR4e4INc4/V3Et4coDvr+2peSvf322yHO5PvgVMnNzU2YK8jnW1//9L2t3/bfW3WrDBGRyy67LMT63OJ/95Zbbin0vqJwkrUlqlChgsn55yq6ZNmPVaYdh8x4AgAAAAAAQCR48AQAAAAAAIBI8OAJAAAAAAAAkUhJjyffG0f3dfrlL39pcuXKlUv4d+rWrRvijz/+2OT0cq2+xlXXk1911VUmp5eT9D2cUtHTqSDq1KljthctWhRi/+/dvHlzsexTSeKXBdWfK5/TY+6X/v7iiy/Mtu5/4Gundc1tqVKlTM7X3MaF7+Oka9QLu9xrjRo1zLbuudSyZcuEv7dw4UKzrZcJ9T2dzjjjDLPdu3fvhD+r+yn4/lxIHX/M3HnnnSH2n6U///nPIR4+fHi0O4bD0r2b/v73v5uc7svme8bosXv33XdNLllfNt1DT0Tk9ttvD7G//r7++uuHjUXo05YOureLP796us/X3LlzI9unOPCf+7POOivEvifq+vXrQ+yPu+Lo65SMPib98t6///3vQ+x74Pj7j0yn++g89thjJtexY8cQX3jhhfn+m/oz4vtgar5PX6roe7Z69eqZHNdu+xnu0qVLwp/zvYn1/VKrVq1M7uyzzw6xv9fV33krVapkcv57ij5nTJs2zeTGjRsX4t27dyfcb0Qj2Xcv39PJ90bU/aL9/ZC+B8sEzHgCAAAAAABAJHjwBAAAAAAAgEikZM6rnz6my+n89LFkKlasGOImTZqYnF6uVZdZiYj06NEjxLpc73D7lk5+aUv9b/KlZHHlSzH0v9tPM9dL0+rPhogtmfNTkf3yorrkx38e9JTmAwcOJN33bOdLp3TZzjXXXGNybdq0CfGWLVtMTk/3ffLJJ01uyZIlIfZT9Pv27Wu2H3744RC3aNHC5PRniVK76NSvXz/htn/fdRlCustEsoU/ZnVZgD5GRUT27dsX4nfeecfkRowYEeJk07obN25stq+88kqzXaZMmRDPmzfP5HQ5H6V1xU+PjYjI3XffHeLq1aub3J49e8z2b37zmxCvWLEigr2LD19q17Rp0xD7thW6BcOGDRui3bEi8Nd4XeruyzT9vVym0985/P3l8ccfX6i/6UvYi5u/19JWr15djHtSMukWLr6VhD6+TznlFJObMWNGiJs1a2Zy+ruybi0jIjJ27NgQf/LJJybnf1aPj7+O8h2n+Onv/c2bNze5c889N8T+Grtp0yazPXny5BB/++23Jvfjjz8WeT+LEzOeAAAAAAAAEAkePAEAAAAAACASPHgCAAAAAABAJFLSAGnbtm1mW9ecbt261eT8suua7vej+8eIiNx6660h7tSpk8npuviC9Ery9a66HlYvXSgi8v333+f77yZ6DV3fKyLy4Ycfhnjz5s2F+vuZQNc8+6VHdb+uq6++2uR0bwBfq6xrXFetWmVyU6dONdu6h0K3bt1MTn9e9u/fb3Lbt28X/D9ly5Y12wMHDgyx7sclYns8DBkyxOR0jydfx6z7x/g+QH7JZj3uJ554osnpZdz962daPXRJo4/ne+65x+R0jwvd90WEvk7p4HsuPfXUUyHWvRhFbB+nO+64w+Q2btyY8DX0mL/66qsmd/LJJ5ttfQw/8sgjJud7PqF4+Z5ffrlv7Y033jDb8+fPD3GmLe1c3HzfU708uu/Jpu+tdQ82EXvv4ntD6b6XnTt3Njm9/Lrvwzd37lyzvXbt2hAn67vm+0+98sorIX766adNrlevXglfLxOvzf6eNhVuv/32EKejT+0ll1wSYv8dzi/zng18X7Z27dqF2H/n1D975plnJvyb/jvftGnTQvzggw+anO59St+mzKLPt77Hk+4B5/u6+XOhfiaRiedJjRlPAAAAAAAAiAQPngAAAAAAABCJlMzh9EupTp8+PcS6rEbETuFMNkVRTxUWERkwYECIjzzyyHzvmy6f8iWBemqjiF2mcvny5Sa3cOHCEBd2qqkv1/vuu+9CHOfp6dWqVQvxtddea3J6Kqovw/zmm29C/PLLL5vcmDFjQqzfR5FDy7dOPfXUEPuSrJo1a4bYL1E5ZcqUEGf61MbC0sfoeeedZ3KXX355iHfv3m1yt912W4gnTJhgcn4p7vxat26d2dafAb+kbdWqVUP8c1NYUTD6M+HLOHbt2hViXaaB4qOvjyeddJLJ6aV99ViJiDz55JMh9udQzZcD6bJ4XyLvywCef/75EOvzq//ZMmXKmJw+hnfs2JFw31Aw+rMyaNAgk9MlPv6eR4+jiEhubm6I/fhkY2mOp+9tzzrrLJO79NJLQ+zviT/99NMQ++uYvpf2Y6evf7Vq1TI5/Xd8GZW+5xIRGTZsWIhHjRplcrp02r+Gvs/z5wDfxiLT5eXlJczlt3xYn5dFbBsD3bpExN6z6uMulfTn0H9PysbSLj8+p59+eoh9GZ7mv9fpkmR9vRWx35WTlbYjs+hzoy6tE7Gld/5z5D87ejvTnxcw4wkAAAAAAACR4METAAAAAAAAIsGDJwAAAAAAAEQiJT2efA2/7qXk65979+4dYl+znqxWtiB9nRLti1+69bXXXjPbs2fPDrHvU6D7MxW2vtLXRmd6nWYifhzPOOOMEHfq1MnkdC8C3ytMLyn65ptvmlyypdl9PbZ+Tf16IrYHmH99v52NWrRoEeK+ffuaXIMGDUI8evRok3vvvfdCnKqeSr431NixY0OseyKI2D4Ifqlp378NBVO/fv0Q161b1+Tef//9EHP8pIdervyRRx4xOb0k++OPP25yemn1ZNcm36dgxIgRIa5cubLJ6WNURGT8+PEh1ucPEZF+/fqF+IQTTjC5f/zjHyGeOHFiwn1Dwehj+aKLLkr4c/7+y9876X5QH374ocn97ne/C3Fh+/vFib/v1ctt+3tp3V/Hj8+tt94a4kqVKpmc7tG2Zs2afO+LP7YfeOCBEDdt2tTk9DHp76v09uLFi01u0qRJIY57v8X89njyPSrr1asX4j/96U8m95vf/CbEvu+m/kwURPXq1c22vq7fcssthfqbmS5Zf9Pzzz8/4e/pc5zvbzp48OAQ++Ni7969hdlNlDD6Wigi0q1btxCfe+65JtesWbMQ+3Ohfwaxffv2VO1i2jHjCQAAAAAAAJHgwRMAAAAAAAAikZJSO09PGfRLca5cuTLEfsnk8uXLJ/ybfgnnRJJNVfbTw9955x2zHfdpv8XFl9rpJbb98q/6Z/0yu3rp0d27d+f79f1rtGnTJsR+arkuB9KfTZFDlxvPBn6aqJ5i7EtFdRnNHXfcYXLFcSytWrUqxLpMVsSW7fTv39/k9BLRHPMF96tf/SrE/rz83HPPhbhKlSomp4/Lhg0bmpxfxltvMwW9YHSZmi6lEhFZu3ZtiMeMGWNyycrr9Dj7MgNdjqtL0kVEPvjgA7N93XXXhViXYIvYMp8rrrjC5P71r38l3Dfkny+ZO+WUU0KsS0tEkt9z6Wu6iP3s+LHTJZxPPPFEvvc1G23dutVs63ILvYS7iC3V0SWsIiKTJ08OsW434TVu3Nhs6/O3iB1nX8Z18sknh9h/VkqXLh3i+++/P+F+x8G6detCXNj7CV9Opz3zzDNm++KLLw7xsccea3L6+PbfhZK57LLLzLb+d3zxxRf5/jtxokvWfVl6jRo1QuzLh4cOHXrYWCR5ixDEk36u4Z9x6O+jvjWFbwmyYsWKEPvvYpmGGU8AAAAAAACIBA+eAAAAAAAAEAkePAEAAAAAACASkfR40vwSgHPmzAmx77fTqlWrhDndQyBZ7wHfp0L3CvE9fApSA43o7d+/32zr/gbJ+o/4XmE9e/Y02x07dgyx7z+l+wTNmDHD5Hy/hbioWLGi2fbHmqaPte+++87k9HLKvkdPcdD9ZPyyxX379g1xhw4dTO6ll14KsV52Opv540L3e2ndurXJ6eXR/Wfn9ddfD7GvQ0/Ww8/3xtA9L3RPLhSNHoOTTjrJ5JL1gtHLe/vltfX51/cQevDBB8326tWrQzxq1CiT03/X9zdAavhj+e677w6xPwdoya6/nr8/q1ChQr5/Nxv491Jv+x4w+r0sW7asyb377rshfv75500uv33xfE+2qVOnmu06deqEuFy5ciane+D4f9PcuXNDPG7cOJPbt29fvvYtU+h+P/78l99eLLq/nYi9F/b3xe+//36I77zzTpPLby9cz/fbW7BgQYjXrFlTqL+Z6XRv2MqVK5tcsvti3W+0IOdNxIO/J9bft3TvOxF7fvDnXt+T+uOPPz7s72UiZjwBAAAAAAAgEjx4AgAAAAAAQCR48AQAAAAAAIBIRN7jyde/3nPPPSFu2rSpyT399NMJc7q+3NfN6npbX+PcoEGDENevX9/kdC3u4f4uCsf3a5kyZUqI169fb3J6fHwdte7No3sxidg+Ad27dze5u+66y2xXq1YtxOvWrTO5f/7zn4fdT5F49QA76qj/d6h36tTJ5GrWrBlif/zosZw2bZrJ6e10vFf6Nb/88kuT0/25TjjhBJPT54GC9HjS700czhX6eHv00UdNrkuXLiHWx6iIrVP34677Z23ZssXkdF36/PnzTc736PK/i/zTfRR1TyURkcaNG4d4yJAhJpesp53uW+B7vST7G/6coXs+ff311ya3Z8+ehH8XqdG/f3+z3bx584Q/q89x/nznr6N169ZNwd7Flz73+WuV7qGj+5yKiJx66qkhvuOOO0xO31v7nh/6ep+sH8ju3bvN9jvvvGO2zznnnBD7Mdb9jPzfGTNmTIjj1tPJO+WUU0Lsr4dvvvlmwt/TY6R76InYvlj+fjpV/V2OOeaYEJ977rkmp3tO+dfPFjt37gyxfw/09VD3+BKx17gTTzzR5IYPHx5if63U4+rPt5ne0yfudH9E38tU33P5nO7f5u+B/f2R75edyZjxBAAAAAAAgEjw4AkAAAAAAACRiLzUzk8Z/Pbbb0Psl3L985//HOIePXqY3IUXXhhivdS3fw2/JLBePlgvjylipwOLMJ0xKrrcw5c36qmH1atXNzlfMqd99tlnIb788stNTk9tFLElUn5KeMeOHUPslzLWJStxmm7sp3vqKd/+eNXTgf20cV9GW9z08Tp9+nST00uT9u7d2+T0tPKvvvrK5PRnQL8vIvZc4kuYMrE0bMeOHSEeOnSoyemSuf/4j/8wOV2W50v0HnrooRDnd0lvpJZebtuXzurx8tdKXYJapkwZk0t2Dn377bdDfMstt5icP0fEqXw5UzRp0iTEv/vd70zOj7O2efPmED///PMmp8dcROTTTz8NsT9vwvJtA/S51pfTPfPMMyHesGGDyekyDU9fGxctWpTw53wZui67F0leVq1L7Xbt2mVy+t/oz0FxKFPX9DXPS3a+a9u2bYj9EuwPP/xw0XfsZ+gSen/Mcp62ZeL6e6uISKNGjULs3ztdpqhjEZHf//73Ifaldt98802It23bZnILFiwIsS/VXbhwodnW911+v/U5w7c2qVixoiSij3VfDqy/V/t2JR9//HGI4/z9Wt9LVahQweT0Z6Vs2bImp8+bvv2EP9/HqWSZGU8AAAAAAACIBA+eAAAAAAAAEAkePAEAAAAAACASaS3G9zWLs2bNCrGvee7evXuIfY8nzdeRxrmuNFPo/jevvPKKyeka14suusjkdG+KRx55xOR03wJfR63rkT3f/0nXWfvPXJzovgq+x4Qeg0qVKiX8Pd/nqiT1atD9SERsT5LOnTub3GWXXRbilStXmtzMmTND7HvC9enTJ8QjRowwuby8vBCXpPclGT2eS5cuNTm97Xv66N/z7wN9ndJP9+eYMGGCyel+EP48OXDgwBD/9re/NbkqVaok/Jv3339/iDdu3FiIPUaUdA9Ev/S35nsGPfHEEyF+8sknTc734PT9wpCYv46++uqrIV67dq3J/fGPfwyxPgZFROrVqxfiZPcuuseI55fo9r0Ldd8Rn+vXr1+Idb8gEZE777wzxLrnnMihfWcyje+LpsdM37P+nL59+4bY9/vx73UUevXqFWL/mfSfw2yk7xt97zP9vVL3ghKxvRJr165tcrqPre9p26xZsxD7e0h9rPneyH5b9+7U96U+57836R6myc7nvoeR/j7ue/jGlb93qlq1aohPPvlkk6tVq1aI/fl28eLFIf78889NLs79MblbAAAAAAAAQCR48AQAAAAAAIBIpLXUzk8n1GUvfuqnnvLrl/fWPvroI7M9adKkEM+dO9fkKMMrHslKP/wSklrPnj1DrKcyioh06tQpxH65Xk+P84oVK0xOlxDMmDHD5PxnMC6OOeYYs52sNFErySVk/ljWy7r6qf4XX3xxiIcOHWpyetpy+fLlTe6TTz4Jsf8cleT3pjD0VGu/xLeeru2X/UXJ4ksffUllop/1y/7qUtZbbrnF5JYvX16UXUSK+TIJX1KRyJo1a8z23/72txD7cltfhkypXeHpY+udd94xuX/9618h9qV2uhTIl7/kl1+aXS/bLmJLwPbs2WNy+lp56623mpxuaXDdddeZ3DPPPBNiXyKfCfy1/t133w1xx44dE/6eL4f81a9+FeJnn33W5DZt2lSofdOv4e9f/X3eGWecEWL9bxA59HjPBv4c1qFDhxD79i76WnnbbbeZnL4X7dq1q8npMc/NzTU5PXZ169Y1Of0dp3Llyibnt/Xns1WrVglz/t+r93vDhg0mp9viTJ8+3eT09yb9fdv/zTgpXbq02dbldP7aqN8D3epAxJZp+nvpuL53Isx4AgAAAAAAQER48AQAAAAAAIBI8OAJAAAAAAAAkUhrjydP1zTqPiIiIhMnTgyx772if2/kyJEmp3u9+KUM41xDWVL5PgG6P4jvJTN27NgQ6yWhRUS6d+8eYr+8rV92Ui8p+tRTT5mc7jmVLUvB+/r9ZMt06mV9fT+Iknz86P4Gfinwdu3ahVgvISsiUrFixRB/8803Jvfyyy+HuDiWO04nXf+v3y8RW5ful3xF5jjqKHv579OnT4h9T4tVq1aFOO6f/Uzn+/2cdtppCX9WX/Puvfdek9u4cWPC36OnUzT8tVj3PdWxyKHX48Lw1/CCXNNHjBgR4qZNm5rchRdeGOJrr73W5PR+jx49utCvny7+PrFSpUoh9v1Gda+rli1bmlzDhg1D/Nprr+X79fX3GN+3SfcG8v386tevb7Z1D9VHHnkk368fV7736QknnBBiP65LliwJse+Lpu+v/TH69ttvh9hff/W9Z69evUyuOM63+nM1depUk9P9h3wvIt0HLq59cT3/DEKf//x3Cn2PPGvWLJPTzzWy6fkEdw8AAAAAAACIBA+eAAAAAAAAEIkSW2rnSzieeOKJECeblrp+/XqznS3lU5lKL+/pp/brpYV92ZNesrJJkyYm58s0dcne5MmTTU5PDfXTlvXnMdOnPeqpwg0aNDC5cuXKhThZmaI/tjLFV199ZbYHDx4cYj2d2pszZ47Z1tNik5Unxt28efNCnOnHBf4fXaLllwyfP39+iLP5s58J/PVPL3ftl3sfM2ZMiF9//fV8v0aysmvK8IpHustadPnt3/72N5Pr3LlziGvWrGly+t5Nf/5EMvN6oktktmzZYnL6HmL48OEmp/+tuuXEz/nHP/4R4j/+8Y8m5+9htapVq5rtUqVKhfiVV17J9+vHiX6/fv3rX5ucLpPcv3+/yemWLr51heaPUV8um8i//vWvfP0c0kOXRYqIHH/88SH2pXbjxo0LsS61FLFlmv4zFmfcIQAAAAAAACASPHgCAAAAAABAJHjwBAAAAAAAgEiUqB5PyezZsyfE+a2TRWbTvUR8j6fZs2eH2PcsKlu2rNm+5pprQlyvXj2T0/0wfH+BL774IsS6v49IZvcO8z04dP8n/x7opVMztQbZj9V7770XYl1/7RVlqek48f9uzr/x53ss6h4g2XocZArfg+u6664Lse8Bo3vSFKR3l+7vI0KPp2ykx3zz5s0mpz9XderUMbl27dqFuHLlyia3adOmVO5isdB9qv7whz+YnO6f2a9fP5MbNmxYiAvSr0ufm6dNm2Zyuk+R7//0xhtvmO2tW7eGOFuv6bqX4bJly0xO9wbVvXhEDn3fkd3058hfR3UPON9/Md19+tKFOwQAAAAAAABEggdPAAAAAAAAiETGlNohu/nSj+effz7EellYEZFzzjnHbB933HEhbtGihcklmyL58ccfh1iX3YmIrF27Nj+7XSLpqZ8itoRu586dJqeXUY9LiY3+d8Tl3xQlXYopcmh5BOJn165dZnvdunUhrl69usnp0mZfEo3080u8p0LHjh3N9lFHcSuZzXyZkm5N0LZtW5PTpXZ169Y1uUwstdMaNmxotl9++eWEPzt9+vRCvYa+Z/F/44EHHkiYa9SokdnW97TZeh+k/92+nYYutdMtJ0QO/T6C7OJL5HTZ6po1a0xOf1Z82w/9/TObMOMJAAAAAAAAkeDBEwAAAAAAACLBgycAAAAAAABEgsJ8ZARfg56XlxfiFStWmFz//v3NdocOHULcunVrk9M9jfzf+cc//hFiv1xwptE1yb6WvU+fPof9ORH6AODQcaePTzz4/gK6b1379u1N7t133w3xt99+a3KvvfZaiB999FGT45wRT1OnTjXb+rrhe8L5noKIH9+75JNPPgnx9ddfb3L79u0LcaVKlUxOf3Yypf+Jvh76PqFnnnlmiN9++22T0+fUwvqf//kfs33HHXeEeOTIkSbn72EvvvjiIr9+nPjP8NKlS9O0JyjpfI/fV155JcT+uN6xY0eI/bXQf9/KFsx4AgAAAAAAQCR48AQAAAAAAIBIUGqHjKSnNC9fvtzkhg0bZrZfeumlEFesWNHk9FRHv4S4XobaT6HOZIsXLzbb11xzTcKf9VNKkX12795ttpcsWZKmPUEq+XPazTffHOIbb7zR5Nq0aRNiXYIsIvL666+HmNK67LBw4UKzrUvfP/74Y5N79tlni2OXkEb+uJ82bVqIH3roIZP78ssvQzxz5kyTy5TyOk2fR1etWmVyjRs3DvGIESMS/l4qXlvEnsP/+7//2+TGjBljtleuXFnk1wey0f79+8321q1bQ/z999+bnD6ncX/0b8x4AgAAAAAAQCR48AQAAAAAAIBI8OAJAAAAAAAAkcg5mI+i6u3btx/SGwfptW3bNqlQoUKR/gbjWvIwrvGUinEVSc/Y1q1b12yvW7cuxJnYkyOVMnlckzniCPv/SellzuPU7y6RuI5rVBo2bBhivXy0iO2VWBJwjS1eRx1lW8nqPiep7HlSEo7ZRo0ame3WrVuH+L333ivKbhWYP4dnan+ZkjCuSD3GNZ7yM67MeAIAAAAAAEAkePAEAAAAAACASBz18z8CAMhWa9euTfcuoJhlalkG0uObb75J9y6ghPrxxx/TvQvFZuXKlUm3ixPncAAlETOeAAAAAAAAEAkePAEAAAAAACAS+XrwlO0rF5VEqRgTxrXkYVzjKVVjwtiWLIxrPDGu8cU1Np44ZuOJcY0nxjWe8jMe+Xrw5JfHRfqlYkwY15KHcY2nVI0JY1uyMK7xxLjGF9fYeOKYjSfGNZ4Y13jKz3jkHMzH46kDBw7IunXrJDc3V3JyclKycyicgwcPyo4dO6ROnTpyxBFFq5RkXEsOxjWeUjmuIoxtScG4xhPjGl9cY+OJYzaeGNd4YlzjqSDjmq8HTwAAAAAAAEBB0VwcAAAAAAAAkeDBEwAAAAAAACLBgycAAAAAAABEggdPAAAACLp06SI33XRTuncDAADEBA+eAACRu/LKKyUnJ+eQ/y1dujTdu4YUeeSRRyQnJ4cHFgAQseeee05yc3Plxx9/DP9t586dUqpUKenSpYv52by8PMnJyZFly5YV816iqK688ko577zzzH976623pEyZMvL444+nZ6dQZFOnTpW+fftKnTp1JCcnR955551071KxyNoHT/v27Uv3LgBAVunRo4esX7/e/K9x48bp3i2kwGeffSbPP/+8tG3bNt27AiCJ1atXy9VXXy116tSR0qVLS8OGDeXGG2+U7777Lt27hgLo2rWr7Ny5Uz7//PPw36ZNmya1atWSWbNmyZ49e8J/nzx5sjRo0ECOPfbYdOwqUuh///d/ZcCAATJ8+HC55ZZb0r07KKRdu3ZJu3bt5Jlnnkn3rhSrrHnw1KVLF7n++uvlpptukmrVqkn37t3TvUsoovHjx8uZZ54plSpVkqpVq0qfPn34f3NioEuXLnLDDTfI7bffLlWqVJFatWrJfffdl+7dQgocffTRUqtWLfO/I488Mt27hSLauXOnDBgwQP76179K5cqV0707SJEDBw5wHo6Z5cuXyymnnCJLliyR1157TZYuXSrPPfecTJo0STp06CBbtmxJ9y4in1q0aCG1a9eWvLy88N/y8vKkX79+0rhxY5k5c6b57127dk3DXiKVhg4dKn/4wx/k9ddfl6uuuirdu4Mi6Nmzpzz00EPSv3//dO9KscqaB08iIiNGjJDSpUvL9OnT5bnnnkv37qCIdu3aJf/5n/8pn3/+uUyaNEmOOOII6d+/vxw4cCDdu4YiGjFihJQrV05mzZolQ4cOlQceeEAmTpyY7t0CcBiDBg2S3r17yznnnJPuXUEKcR6On0GDBknp0qXlgw8+kM6dO0uDBg2kZ8+e8uGHH8ratWvlrrvuSvcuogC6du0qkydPDtuTJ0+WLl26SOfOncN/3717t8yaNYsHTxnujjvukAcffFDGjh2bdQ8rEB9HpXsHilOzZs1k6NCh6d4NpMgFF1xgtv/+979L9erVZeHChdKmTZs07RVSoW3btjJ48GAR+fdx+/TTT8ukSZOkW7duad4zFMXYsWOlfPnyYbtnz57yz3/+M417hKJ6/fXXZfbs2fLZZ5+le1eQYpyH42XLli0yYcIEGTJkiBxzzDEmV6tWLRkwYIC88cYb8uyzz0pOTk6a9hIF0bVrV7npppvkxx9/lN27d8sXX3whnTt3lv3794f/g33GjBmyd+9eHjxlsHHjxsno0aNl0qRJctZZZ6V7d4BCy6oHTyeffHK6dwEptGTJErn33ntl1qxZsnnz5jDTadWqVTx4ynC+T0zt2rVl48aNadobpErXrl1l+PDhYbtcuXJp3BsU1erVq+XGG2+UiRMnSpkyZdK9O0gxzsPxsmTJEjl48KC0bNnysPmWLVvK1q1bZdOmTVKjRo1i3jsURpcuXWTXrl3y2WefydatW6V58+ZSvXp16dy5s1x11VWyZ88eycvLkyZNmkiDBg3SvbsopLZt28rmzZtl8ODBcuqpp5r/Aw/IJFn14IkvOfHSt29fadiwofz1r3+VOnXqyIEDB6RNmzY0jo+BUqVKme2cnBxKKGOgXLly0rRp03TvBlLkX//6l2zcuFFOOumk8N9++uknmTp1qjz99NOyd+9eenhlMM7D8XTw4MGk+dKlSxfTnqComjZtKvXq1ZPJkyfL1q1bpXPnziIiUqdOHalfv7588sknMnnyZGbJZLi6devKW2+9JV27dpUePXrIuHHjJDc3N927BRRYVvV4Qnx89913snjxYrn77rvl7LPPDv9PHQCgeJx99tkyb948mTNnTvjfKaecIgMGDJA5c+bw0AkoQZo2bSo5OTmyaNGiw+YXLVok1atXl0qVKhXvjqFIunbtKnl5eZKXlyddunQJ/71Tp04ybtw4+fTTTymzi4GGDRvKlClTZMOGDdKjRw/ZsWNHuncJKDAePCEjVa5cWapWrSp/+ctfZOnSpfLRRx/Jf/7nf6Z7twAga+Tm5kqbNm3M/8qVKydVq1al3BkoYapWrSrdunWTZ599Vnbv3m1yGzZskFdeeUWuvPLK9OwcCq1r167y8ccfy5w5c8KMJxGRzp07y/PPPy/79u3jwVNM1K9fX/Ly8mTjxo3SvXt32b59e7p3CYW0c+fO8H/YiYisWLFC5syZI6tWrUrvjkWMB0/ISEcccYS8/vrr8q9//UvatGkjN998szz22GPp3i0AAIAS6f9KYLt37y5Tp06V1atXy/jx46Vbt27SvHlzuffee9O9iyigrl27yu7du6Vp06ZSs2bN8N87d+4sO3bskBYtWkjt2rXTuIdIpXr16kleXp5s3ryZh08Z7PPPP5cTTzxRTjzxRBER+c///E858cQTY38Ozjn4c8XeAAAAADLeypUr5b777pPx48fLxo0b5eDBg3L++efLyy+/LGXLlk337gEAYooHTwAAAEAWGjx4sPz3f/+3TJw4Udq3b5/u3QEAxBQPngAAAIAs9cILL8i2bdvkhhtukCOOoAsHACD1ePAEAAAAAACASPB/awAAAAAAACASPHgCAAAAAABAJHjwBAAAAAAAgEjw4AkAAAAAAACR4METAAAAAAAAIsGDJwAAAAAAAESCB08AAAAAAACIBA+eAAAAAAAAEAkePAEAAAAAACASPHgCAAAAAABAJHjwBAAAAAAAgEjw4AkAAAAAAACR4METAAAAAAAAIsGDJwAAAAAAAESCB08AAAAAAACIxFH5+aEDBw7IunXrJDc3V3JycqLeJyRx8OBB2bFjh9SpU0eOOKJozw0Z15KDcY2nVI6rCGNbUjCu8cS4xhfX2HjimI0nxjWeGNd4Ksi45uvB07p166R+/fop2TmkxurVq6VevXpF+huMa8nDuMZTKsZVhLEtaRjXeGJc44trbDxxzMYT4xpPjGs85Wdc8/W4MTc3NyU7hNRJxZgwriUP4xpPqRoTxrZkYVzjiXGNL66x8cQxG0+MazwxrvGUn/HI14Mnpq+VPKkYE8a15GFc4ylVY8LYliyMazwxrvHFNTaeOGbjiXGNJ8Y1nvIzHjQXBwAAAAAAQCTy1eMJAHzDOP1k+6effiru3cm3o45KfJr78ccfi3FP4i/Ze11YjBEAAMgG/l47FU24Pe6rkC7MeAIAAAAAAEAkePAEAAAAAACASFBqp+jpjL5B1sGDB0N84MCBYtsnpE82fh78v7N69eohPuOMM0yuYcOGIR43bpzJ6dK7bdu2JXy9ihUrFmo/RUR27doVYj9tuHbt2iE+++yzE/6NKVOmmO0tW7aEeMOGDSa3Z8+eQu1npqtcubLZ7tOnT4ibNGmSMFehQoVCv6Y+pl5//XWTGzVqVIjnz5+f8PdQPPR5siAlAX6sGLt4SFZuy5gj2flC31eV5PL9qOn3xV9H9Xu0ffv2hLmo6HvEqlWrmlz58uUT/t769etDvHfv3tTvWDEqU6ZMiP3S8fp+V9+jiojUqFEjxP48eeSRR4a4VatWJtemTZsQF6XsTn9epk6danJ6v7///nuT27x5c6FfE/CY8QQAAAAAAIBI8OAJAAAAAAAAkeDBEwAAAAAAACKRVT2eSpUqZbZ9f5mOHTuGWPevERFZunRpiH1fkZUrV6ZoD1HcdF310UcfbXK6N1CDBg1MbuHChSGeNm2ayWXyMqX+c//GG2+E+KSTTjI5/d4NHTrU5HSvgWQ9nipVqlSY3RQRkZ07d4bYv+e614Cux/f87+3fvz/Es2bNMrlu3bqFOJv6T/jz5FNPPRXi3NzchL/n+4Vpvl+W76dVs2bNEN97770md/vtt4fYf+6ee+65EH/77bcJXx8Fo8fSnyf1cdG0aVOT8/1JdE8ffx1dtmxZiP3nYdOmTSEujj4msOdNf5z7vm/6fNu5c+eEf9PfK+lrp79O6HMxMlfZsmXNtr6vOuGEE0xu69atIX711VdNTvdfjBvft0cfQ5deeqnJ/fDDDyF+4oknTK44vovovk5DhgwxOT+e2kMPPRTi9957z+RKYq83fc3z98VXXHFFiC+55BKT098NVq1aZXKdOnUKcbJ7J39frLeT3Vf9HH1O9fdHuu/WjBkzTO73v/99iLO11ylShxlPAAAAAAAAiAQPngAAAAAAABCJrCq1q1u3rtk+99xzzfYdd9wRYl8ioKdP+uW9delJJpdZFQc/pVhPG/UlFFFMv/VlIt27dw/xiSeeaHKXX355iMuVK2dy8+bNS/gac+fODXGmTw/X45NsGddkS2j7JXeLMlVYS7Z0r/4s+aVh9WfAL+urpz/XqVPH5FK135lGL4MsIvLf//3fIe7Tp4/J6c9B27ZtTU6Pw/PPP29yo0aNMtu61EBPaxcROe6440I8aNAgk9Pn+Ouuu87kSuJ0/pLKf9Z12ZU/Lvr37x9iveyzyKFlmnoM/DlUl97pc6iIyIQJE0Lsj1lK7/LPj2v16tVDXKtWLZPT49q6dWuT88d26dKlQ6zLZD1/Ltalsr78Zvny5SHm2D2Uvh77+xo9BnoJdZHU3ZPo1/f3Bro1gb6P8tv169c3ua+//jrE48ePN7lMv5dKxr9/Z555ZogHDBhgcvo7hi+tK47vIrrsS5eOiYg0adIk4e/pMrxx48aZXEk8vvV9qy/3/+Uvfxli/92gefPmIfbXJt2ewtPvQUHeDz/OuhTO3yPrdjP16tUzOX3v5luL6PuzTCiTxP+jzy1+XM8777wQT5061eS++OKLEKf6HosZTwAAAAAAAIgED54AAAAAAAAQCR48AQAAAAAAIBJZ1eOpSpUqZtvXuOqaXl0LK2J7PiXrLYPk/DKhuqfOrl27TG7Hjh0h9n09CkLXLvvX1z1qTjnlFJPT/WJ8bwzd48L3NVmxYkWIM60vge/n88wzz4T4scceM7lq1aqF2C99vWbNmhAvXbo04e8V5FjyteS61lx/VvzP+iVt9XnA9xzp2bNniP1yt9naS8Yfew888ECIH330UZObPn16iH3fCt1rZOTIkSb3r3/9y2x//vnnIf7HP/5hcg8++GCIBw4caHK6Zl33jxE59HMIS5/jfK+O9u3bh7hHjx4md8EFF4TY95pJ1hetWbNmZlsfexs2bDA5vSS7X+q5OJYQjwvdi0nEjqtfCl33ePL9nypXrmy29Tj7fn/6vOn7/TVu3DjEjRo1MrmtW7eG2Pcp8tebbKCvmyIixx9/fIj1sSNie++8//77JvfCCy+E2PecSdar0f9sq1atQuzPF/r1zzrrLJPTPXF8f5pFixaFeNu2bQn3JW789w3dG8+Pid72vWiLg/4cJOvZ6iXrEVoS6e8KHTp0MDn9Gfb/5mT/Tt1/aeLEiSb34osvhlj3FBYR+emnnxL+TX8M7du3L8T6uunpc6+I7bvpz8UnnXRSiDOhP1cyenz82CV7nzOFH3N93+uv48mMHj06xPoeT6ToY55ZZwIAAAAAAABkDB48AQAAAAAAIBKxL7XTy0D37dvX5H7xi1+YbT8NPRE/PU9v+1y2ludoemrupZdeanK9e/cOsS/z0lNR/XLruvzHv8d+qqteml0vUytiS3N8+UCypU/1z/ryvWTT1Us6X1Y1c+bMEOvSBxE79X/t2rUmp5eyX7Bggcnp46wo75V+zWSlF/7zoY9Rnxs7dmyI/ZTVOEzDTTX/nvjyRE0v3d6xY0eT86V22saNG832PffcE+LTTjvN5PSy73/6059M7te//nWIM216eHHQx6UuwRKx5XU+p8vrCnL98+dpfbzpMmcRkW7duoV4586dJqdLaRnXQ8dAj6svJ7///vtDfOyxx5qcHg9/fvXne/2aderUSfj6/h5LLxXvSziHDRsWYl8u9s0330g2OOaYY0Ks75VE7L2LL7XTpVu+DE7fA/nSx4KUvuv7Hv97vuRW0y0VPvzwQ5PT5T7fffddvvcl09WsWdNsd+nSJcT+PrS470P86+sST3/vq8/pmX4u1m0Y/FLz+nznr1U//PBDiP098yeffBLihx9+2OSWL18e4uJ47/y4JruHzvSx1B555JEQ+3Nqu3btQuxLGEsyff6dNm2aydWoUSPE/nu05q8h/fr1O2z8c38nP5jxBAAAAAAAgEjw4AkAAAAAAACR4METAAAAAAAAIpG5zWgS8H0jdM8R3f9DpGBLC+7evTvEeklMEVvPrpfZ9D+rl7kUiVfdbDK6Htr3UapXr16IfY8J3afAL6Gt+0H5vkS+j8Spp54a4nPOOcfk9LK1yXo6ebr+1/ccyaTa4Khs2bIlxOvWrUvjnhTMpk2b0r0LGSXZ8snenDlzQux7eyTj+wR9++23IX7nnXdMrmXLliH29fu6nwnjfOj5Tp8L/XlS93XyPS2S9Uzbvn17wtf3S4Hrv+N7xLRp0ybE8+bNM7k49RVJBf/e6fsc30dJj6XuJyRi73n0MSciMmbMGLOte/X96le/MrkqVaok3FfdR0q/Xrby51PdL8sfk23btg1xsv6k/v3Xfb78Z0XfcxV0XzXdh0j3yhGx/bn8+Xvu3LkhzqZj2Y+f7tlSkGtsFPxnQn+P8j0d43Qu1j3G7rrrLpPTx1SvXr1MTn++fX9TfV/se5gVRz9g3ZfV99nU3818H7H58+eHONPHVfeE/OMf/2hymfLdzZ8TZs+eHWL/Pfr8888P8ejRoxP+Tf88ZM2aNSF+7LHHTO7dd98NcWF6zjHjCQAAAAAAAJHgwRMAAAAAAAAiUeyldsmm5aeCnzpcv379EJ900kkml2wKuN83Pb1Ql3OIiAwePDjEeiqjiF0K3C/JqUsG9PS/OPOlkHqKsZ8iWLt27RDrqYQidjlHv9x6s2bNzPbFF18c4kaNGpmcLgPzY65f35el6NIDP66+LCGT+CmcDRo0CLEui/D8csCdOnUKsZ4+L5I501nx8/xyynrbT8EdOnRoiL/++uuUvL7/LOlj2O+bLiXL1lI7fXw3btzY5M4444wQ+6V19XUt2fLeupRA5NAp2np8brvtNpPTpZD+OqHHzo9ruktR0sH/m/V9zwUXXGByurzuvPPOMzn9Pvt7kJdeeinEukxW5NAlm3VJWPfu3U1O32f5Mo2JEyeGeOzYsSany7C2bdsmcaXHskmTJiZ33333hViXTIiIlClTJuHfTNYKQB+//l4l2ZLq3o4dO0K8cuVKk9P3tl9++aXJ6fuBtWvXmpxvmxBnehx02aTIoS0p0ql58+Zmu3///iFO9hn0n6VkZdclkb5Wbd682eT09rJlyxL+XrrL0nTJpojItddeG+Lrr7/e5PR52h/P06dPD3G6/01F9fTTT6d7F4rs3HPPNdv6u7MugzvcdiL+e7QuBfXXJf1cxX9W8oMZTwAAAAAAAIgED54AAAAAAAAQCR48AQAAAAAAIBKR9HjSNet+KU7dJ2bPnj0mt2/fvhAXpI5U9ynw/R90bWKypT8930NB98PwfZx0jwu/LKr+N/reGLr2fvXq1SZXHEtrljT+Pdfvpa9V1ss3+2UgTzjhBLOte4f4JZt1XwnfL6Zfv34h9kuI6/p130Mhk3sY+c+d7oWjj0/PH+d+qXTEk+4BJmJ7U/jznT4ux40bl5LX931pdu3aFWJ/zshGyXoBnXbaaSanl2vXPZVE7LUyWf9Dfy6cMWOG2dbnxssvv9zk9PXZ9w7R55dy5cqZnL4WFKRHTSbz9xn6Gqh7OomItG/fPsTHHHOMya1YsSLEM2fONDndY2nDhg1JX1/fE+nxELGfF38N0b2jPv/8c5PbunVriAuzZHOmaNiwYYh1TycR29fJj53m+3O8/vrrIf7iiy9MTp8D/HtekD48+ljW510Rke+//z7EfuwyvUdMquieOhdddFHCnKfPcVH1TdLnX93TSeTQnk+JLFmyxGxPmjQpxJl8j+yl+9zk+xofd9xxIX7xxRcT5rwxY8aE2PdB8ucXFD99L/f444+bnO4zpvt4ieT/WYI/L+u+Xr43pL/GFxQzngAAAAAAABAJHjwBAAAAAAAgEpGU2unSJj+dX0/b1Mv1idip3gsXLjQ5PZ3RlwHoco/OnTub3PHHH3/Y/RI5tNROT2Xz09N0CYlf6lT/rC8f1MsAJ1uSMxtL636OHg8/tU9PL/3FL35hchdffLHZXrNmTYi/+uork3vyySdD7JcC11MPf/e73yXcTz/VNk5jqZc79lP2dYkAskf16tVDfMcdd5hcUafgFlSjRo3MNuV1lh4rEZEOHTqE2Jf11KlTJ8S+dEaXWvlyNr2tz7WH29bX/IkTJ5qcLrXTy/WKiNSsWTPEXbp0SZjzrxenkg59PfQlE3oqvJ8Wr0u0dGmdiP0M+FI7/bO+tO6mm24y27169Qqx/hyJiPzwww8hXrp0qcmNHDkyxF9//bXJpbuEJSq63YSIyMCBA0OcbOz27t1rcuvXrw/x3/72N5P761//GmJ/n63F9T3OBPocd9JJJ5mcLlP357Bvv/02xFOnTjW5wo6nL8lu0aJFiH2pnS+D1vS+fvjhhybnS++Qf/rz4L/H9uzZ02zrc3Pbtm0T/s0vv/zSbN9///0hnjdvXmF2ExFq1qxZiHULIRF7P5Cqskj92Vm+fLnJ+fuIgmLGEwAAAAAAACLBgycAAAAAAABEggdPAAAAAAAAiEQkDTkqVaoUYl+7PGDAgBD75bb18rl6OVbPL92ul1f2/Zd0Hyff08lL1ptH1077Hhe6N8GoUaNMbv78+SGeMmWKyen+TziUHq82bdqYnO7V5Jee9ctt62WhR48ebXJ+OXZNLw3ul4HWy9jGqY+Ip4/DBQsWmNy5555bzHuDkkCfY5P1EPD9JvS5kCW1i0etWrXMdrt27RLm9HXV9/vRY+evzfr8p6/hIiI7duww2/o8miznPx+6d5jv46X7D/leJZnML5Ndu3btEN96660md/rpp4fY31d98803IfZ9vXSPJd+fMtm+VKtWzWzr9933f5gxY0aIfV+vxYsXh9j3MIoTfS9z9tlnm9zll18eYn/vovkeofp+8pVXXkn4s5xrSwbf/7Br164h1sf2z9HnW/9dRPfd/Ll+i/o8ofvriYjceOONIdb9ngrCn4t/7vsX/h/fR6t79+4hvuyyy0yuY8eOZlv3dfRjoL+3vPXWWyan+/jQ+63kefTRR0Psey4tW7asyH9f3xuK2D5St9xyi8kV9fPBmQAAAAAAAACR4METAAAAAAAAIhH52td+eqWe3umnguplIn3JXH5foyjTOfWUZD89ecuWLSHWS0uL2PI6X2qnf1b/jcO9Biw9lk2bNjU5PS3QL0/sSyb1++7HQE9b9tNb9bR3v7y4Ll/wuTiNq34v4/TvQuH17t07xH7Ze2316tVme/z48SEujs+SL/vKltJmfV3t1q2byenyWH++0+fG5557zuQ+/vjjw/6cSPJp174MWZ/T/XLOxx9/fIj99T/ZvUHr1q1D7Md806ZNCfetpKtYsaLZ1te80047zeR02aQupxCx7/OsWbNMTpfXJWs1sHv3brM9efJks71mzZoQ+7KdOXPmhNgv0+1L2ONKl7z4pbBr1qyZr9+rUaOGyenzsPfkk0+GWLclEIl3SWNJo7/v+JYQHTp0CLEvZdX8dxr9d26++WaT0+dQfxwm49uX6NK/ZPvm6WtP3759Te6jjz4K8bhx40yOe0s7zv66/dhjj4W4cePGJudLq5M55phjQnzRRReZnD6Hv/feeybny3wRPf98pFevXiF++OGHTa6wpW/6HvDFF180OX2d8KXcRcWMJwAAAAAAAESCB08AAAAAAACIBA+eAAAAAAAAEIlIejzpvg6+H4SuFfW9GnQtcRRLb/oeBr7WXS/1+/XXX5vcJ598EmK/dKH+2WRLEmcrXT/u6859bXkixx13nNnWPS3y8vJMzi8TqvturV+/3uT00qPNmjUzuXPOOSfEvq+X7pXh+4rEqV5d1wBXqFAh37+nf1bXlYsc2i9E8z1hkvFLemv6vBOn8SgJKlWqFOJkx69f6tlvR+37779Puh1Xekz0eVLEjp2/Hu7YsSPE/hqn3zvflydZbyBPH4u6T56IyIoVK0J8wgknmJw+T5cvX97kGjVqFOIvvvjC5DKtx5O+B+rZs6fJnXfeeSH2fT70edP3T3nnnXdCrN9jkfyPnR9z/xoTJkxI+LvJemdmC92DY/To0SZ3yimnhFiPsYj9PPheLrrv2SWXXGJy+vjR/Z5E7Nj546MgxzJ+nu7HdMYZZ5jcmWeeGeJk33eS9Xi67LLLTK4g/X5SwfeW0d9/5s+fb3I7d+4sln3KVHqc27Zta3J169YNsT9G/f30t99+G2LfP06fT9q0aWNyjzzySIj1NdXn6BFXPOrVq2e2dc+nGTNmpOQ19D2G7g8nIjJy5MgQb9y4MSWv93+Y8QQAAAAAAIBI8OAJAAAAAAAAkYik1E4v5+unW86cOTPEfjqhXppbLyPrtws7ndRPEVy7dq3Z1iVZfr91WYAuSRChvO7n6NIIP4WzXLlyCX9Pj7lfWrJs2bIhXrx4sclNmjTJbOvyOl/KpZfi7tixo8npMhVfwqGns8Zperqffvvss8+G+D/+4z9Mzo+J9vvf/z7E/fv3NzldcuWX/p4yZUqI/XHmyzQHDBgQYl/CoacG63+DSOGXHs1WyZZ19Tk9Dn7p9FSV2egp6X56cLKShTgdp5ouhxWxJcO/+MUvTK5OnToh9mXws2fPDvHq1atNTl87U/U++tfQ5ey+lExfN/xnTl9fkp2TMoE+x3Xp0sXk/Gdd27ZtW4h96bk+DlN17vPHcraW0BWGL9t///33Q9yiRQuTq1ixYoh1mayI/dyXLl3a5Jo2bRpiX76nWwP4EknKaFJL39/6e19/P5NIOo41fe/tz/d6+7vvvjM5fa/tS0oXLFgQYs4Xh9LfTYYPH25yb7zxRr5+T8SWRflS+3bt2oW4c+fOJnfppZeG+O677zY5fT655557TG7lypUJ9y1drr/++hD7dhBPPPFEce9OoZx44olmW1/j/XfcwtL3gA888IDJvfnmmyHu1q2byel7jMK00GDGEwAAAAAAACLBgycAAAAAAABEggdPAAAAAAAAiEQkDRF03eDUqVNNTvdK0v11ROwSsL5XQ+XKlUOsa1FFbO277/Ghexr4ena9zLCIrWn0fZvi2h8kCr6mtlWrViH2dat6XD39nu/atcvkli5dGuK33nrL5JYsWWK2dd8C/7nSS9pefPHFJlehQoXDvp6I7U8Sp3r1c845x2zr8dJLsYok7wOgf9b3NkhG16D/HD2Wvs5YnxN8vzgUjH//9HHh6WNB93TwuaLQPf58T7JkPZ7iRP87fa+QY489NsR+OWV9btZ96kTsePn+alGc4/zy2qtWrQqx7xuhX9/3tNB/x+cyjT5vtW/f3uR0D0zfn2v69OkhHjdunMn5PixIL39vqZet9stk6+uovo8SEWnSpEmIO3XqZHJnnXVWiPv06WNyDRs2DLG+HxcR+fLLL802971FU7Vq1RDXrVvX5Px9sqY/IxMnTjQ5fS/qx0f37fHXaf96en+S7Yu3adOmEPteQJ9//nmIFy1aZHL0D8s/f3732/m1fPlys62vq/5co/vJ+e9Cuk+c/341dOjQEJeUfse/+93vQqzPkyIiL7/8cog3b95cbPskYntlidhetK+88orJ3XfffWY72b7qe2L/nVqfgy666CKT69evX4j1+IuIXHvttSG+8cYbTU73m9S9QfMrO+7SAQAAAAAAUOx48AQAAAAAAIBI8OAJAAAAAAAAkYikx5OmeyyJiKxduzbE27dvNzldE1y6dGmT0/0N+vbta3K6NtHXPOua0zlz5pic39Y/S217weieI762vEuXLglzyXqy6L49y5YtMzldS75hwwaT27dv38/v8GFe3/d/0r1DPvvsM5NbsWJFiOP0WfE9H3RvrXLlyplcsn+3rudfv369yem/4+uR/Rhovn/L7t27Q7x48WKT08d2nHpwpYPvTVGvXr18/V5U77u+FujYmzdvXrHsTzro85avzdd9r/zxpY9Z3ztk7NixIU53rwbfV0zfR/jeVLqPpM9lGt2rwV8b9Xviex7q65Hvz+XvwVCy6Gul722m+d4q+vMxatQok7vqqqtCrHt1iIi0bNkyxDfddJPJ3XHHHWZ748aNCfcHhypbtqzZ1t9VfP9Mfa/jr0363HzLLbeYnL6f8r+ne/r5e6nGjRub7eeffz7E/jqabN90P7l3333X5HQ/uWw97+jj0vdF1ePjv/8Wto9TQeix9NfKIUOGhLhKlSompz/H/vv3Sy+9FOJk56/i1Lt37xD742Dbtm3FvTuB/w6jeyzpPloih17/t27dGuL33nvP5H744YcQt23b1uQaNGiQcH90X80HH3zQ5KZMmRLiVI8rM54AAAAAAAAQCR48AQAAAAAAIBKRl9p5elqxX15TLxfol4HWS8n66ax6CrpfOlgvGamXURSxZX8i8SqZKm7HHHNMiBs1amRyAwYMCLFe2tHz5QN62djrr78+YU4v7yqSfBx9CUeyUj899XXmzJkm5z87cfH++++b7ZtvvjnEDzzwgMn5ZUq1adOmhfjXv/61yekSOT/FWy8H7OmSGhF7rPtyy3SXCsWJn6qsl172x5OeDuyX5i4sfy3QZST6vCNip/f75WnjVGqXjD6n+fHR/PlWlxYXB18iqEtB/DVE86XUurTMT2Uv6fz1R5dBVapUKenPIrv485fe9sum/+UvfwnxaaedZnJnn312iHv06GFyo0ePNtu6lCpbzp8FpY9L/d6KiFx22WUh9iXrmn9vdasA3/5At6DwdEmMP/f7863m75n1ti+Z09d1fb0/3M/GlS6Jbtiwocm1a9cuxKeffrrJnXrqqSH+29/+ZnK6ZK04+DFft25diH1rE618+fJmO1l7jHQpKSV/nt+vFi1ahFifs0UOPZfk5uaG+MQTTzS577//PsS+ZHPo0KEh9s9A9Hex4sSdDAAAAAAAACLBgycAAAAAAABEggdPAAAAAAAAiESJKs7UtdK+Nvaiiy4KsV/qUf+er2+cPXt2iFevXm1yyWqlkZxfJlQvBen7BiRb8lz3+XrnnXdMbvz48SH+/PPPE/7ez9G17uXKlTM5vdSk7xfz6aefhjhbatn9+6rHxI+5rh2uVq2ayen3tUyZMianl/D0/bmS9QXKtP4t2UjXmi9cuLDQf0f3UPDLvPplZzV9/tfLPmcT3S/E93HQ50Lfq0H3ENDvv/+byfrk+VzlypXNdq1atUL8y1/+0uQ6dOgQYr8ksF56ev78+Sanz81xPS97vu9Zly5dQly7dm2T0+fbknYO1Z+XZP3Iksm03kP+2NLHnV4WW+TQfmaJ+Guz7jvjPw/6faZvWNHp975Tp04mp++DkvXC8d9F9HW0sL1n/efM75s+hyTbN3/vq+/RsuV86+9hu3fvHuJ77rnH5PT3Hd+PWN9P+56lxc2fb5s1axbis846y+T0OXbBggUmt23btgj2Ljvo3nzjxo0zOf8MpEaNGiH214lMuwZy1QEAAAAAAEAkePAEAAAAAACASJTYUju/nLJeatlPIdX00soidop+Yaes4t/0+Ojp4SJ26Ue/fG/p0qVD7KcE6vGaNGmSyc2aNSvE+Z1yXlB6uqn/fOilL/1S45k2tbGw9HKbfmqwLuHwpXa6xKZ169Ymt3Tp0hD797GklYKg8AoyDd+f7wcOHBjiyy+/3OR0acOePXtM7r333gvx5s2b8/36mcwfM/pc5XP6XKyn1ouInHDCCSH2Zen6PODLevQy3f7afPzxx5ttfS4499xzTU6XBPmlv/X+rFixwuT0vzfTzst+f/U51d/L6HNqqVKlTK5SpUoh9q0IdDnzrl27kr5+KiQr2/HlHbr03f+e/ln/udLHvS5LKql0qY4u0xER6d27d4hHjRplch988EGI/ZhXqFAhxD179jS5m266KcTNmzc3OX2f880335icL3XPtOMpHXTJmi55FUl+LOhx+Prrr01O3wsX9p7If17q1q2bNK/pcf/4449NTpewx/nzoe9pdVsJEZGLL744xL5Fh77veO2110zu7rvvDvHGjRtTsp8Foc+pvpz9xRdfDLG/butr7ogRI0zuu+++S+EeZhddiunvc9944w2z7b+DZjJmPAEAAAAAACASPHgCAAAAAABAJHjwBAAAAAAAgEiUqB5PuubY1yNXrVo1xL4Xj14CfuzYsSaXilpp/FuypbjbtGkT4mTL9+p+EyIiixYtCnFeXp7JrVmzJsTp6M+VbNnhZL2h4mr9+vVmW4/XSSedZHK654j+bIiIjBkzJsRx7hEAS/dp0edzEZEHH3zQbJ933nkh9j2FtIkTJ5rthx9+OMRx/mzpa9mGDRtMTl/zrrjiCpNr2rRpiLt162Zyujdfr169TE73rfD93Fq2bBlif57UfYlE7HXD9xjRS4r7Jbw/+uijEPtec/r6n+l0j6dp06aZXMeOHUOse16K2Pslv7y37h/zySefmNzChQtDXJCebL7nkh5Xv2y77kXkPx/JlpvXn+tkvdx0L0iRknE99ues/v37h/j+++83Of3eTZ482eR0r5V+/fqZnL6u6s+GiEj16tVD7M+Dy5YtC/ETTzxhcmvXrhWkjv4s+v5m+l74ySefNLnFixcX+bX9fXhB+k/pc8G8efNMzp+b40rfw3bo0MHkdF8nf256/vnnQ/z000+bnP/+EzU/xrqX5q233mpyxx13XIj9OePll18Ose/FG+f7rKhde+21IfZ9tXQfsbhhxhMAAAAAAAAiwYMnAAAAAAAARKJEldrpKcd+eW297K6fSq2XGdRTx0VEvv322xTuYXbT77tfllmXCDRs2NDk9LTU+fPnm5wuJ/DLRxfHFM5kU6F1WYpeMlfETkmPU6lHMroURuTQ8dL0e+mXRtclNpS/lgzJSkmTTcn39Ngee+yxJnfXXXeF2C//XaNGjYR/0y/Xq8uFfInC8uXL872vceGPS12iNGfOHJPT11hfrlWlSpUQn3nmmSanz3G+jEgf3/4c6rc1P656v5csWWJyuoTel37Eaaq/Pqf60nNd+uGvsaVLlw6xLwtp3bp1iP2YL1iwIMQFeR/9+ULfn/mSntzc3IR/R5fs+TIUfa/glx7X19+SUFrn+X+zLmv1bSS2bNkSYn9s6ZJjHYuI1KpVK8S+dFnzx9n06dND/Omnn5rcvn37Ev4dHJ6+F9bLzovYckh/zKxbty7E/jydinHQ54vDbSej78u2b99uciXxeIuaHztNlzKLiPzzn/8McXF8//T3Z/o6fvnll5ucLoP2pfb67/hrz4gRI0L8ww8/FHpfs52/H9LXSn+eXrlyZTHsUXow4wkAAAAAAACR4METAAAAAAAAIsGDJwAAAAAAAESiRPV40r0idF8CkfzXJ/v64zj1f0g3/V76JVX1Ete+v0G9evVC/O6775qcXhbZ17imSrLeVLomX/cKExHp2rVriPv27Wtyo0ePDnGca3GT8bX/mq4X79y5s8npflnZ+t6VNL169TLbumdIixYtTC5ZvwPdv+S5554zOd2TxPcy8fQSxe+8847JPfrooyH2PZ2y8Xyvl74WEdm2bVuIP/zwQ5PTx6UeDxGRMmXKhFj3iRAp/Pvqe5XoHka6v5+I7enjewHOnDkzxPrfFze6l9a4ceNMTl9zfY+nOnXqhLh69eomV61atRD73pnnnntuofbTf+Y038dCj5fvC6jvGyZOnGhy+l7B91Es6b0B/f2q7rule16K2P52Dz74oMnp83Cyc6Z/fxYvXhxi3wdPf658X61s7N9TVPq+dfjw4Sbn+9Fpuq/TV199ZXKFHQfdM0338xE5tE9pMro30ZQpU0yupB97UUh2/fN90tasWRPigvTH1OdNf/7Q3419z1TfU0+fa3wvTX0O8b0h9Wf16aefNjnu0wtP9wr2Y6W/V44cOdLk4tw7mBlPAAAAAAAAiAQPngAAAAAAABCJElVqp/mpjcmmd+qlhP1StXqZXz+tGIXnx2fu3LkJc5UrVz7sz4mIfP/996nfuST8FGZdvqCX8xY5tGQh2/ljUJdJ+JyeYqyXcPc5pI+eln/33Xeb3CmnnBLiZKV1XqlSpULsy4E0XUonIjJr1iyzrfdHl1mJJC/zgZ2i/fbbb5ucXkp9yZIlJqeX/taxiF1C2U+7X7BgQYj9ud+X4+rf1fsiYsuwfIleNo65Lyn88ssvQ6xLvUVETjjhhBCfc845Jqfvj3wZXH7566YfV13C7ksE9OfDLzevryG+LEl/5jKtBMxfD9evXx/iBg0amJw+Z/pyKP3v9vev+n5l1KhRJqe3ddmdSLxLONJBn/N8WZovJ070e6kqEdefl2XLlpmc/gyKiBx77LEJ/87mzZtDvGXLlpTsW6ZZvXp1iB955BGTe+yxx0J8+eWXm5w+//pytvzypXZ625/D9X2ciL1Wrlq1yuRmz54d4ldeecXkdKmzPveiaG677bYQ+1K7ESNGHPbn4o4ZTwAAAAAAAIgED54AAAAAAAAQCR48AQAAAAAAIBIlqtmKriX2y5LqPkG+54iuj9ZLlIokX/Idhedr0nUt+8cff2xyuibZ/15xL3/uey+89tprIZ48ebLJtW/fPsR+WXJfL5+N9DLCvv+E7uuk32MRu9ws0kf3Ahg2bJjJ6WW9fd88vSRvst4rvo/TwoULQzxmzBiT07XuIiJr16497H6iYHw/F33eeuedd0xO99hp3bq1yekePr7Hkx5XP1b+fKv/ju8dwjhbfuz0+657jIiI1KpVK8T9+vUzOX0uLki/Ns1fp+fPn2+2dS8R35tKb+/cudPk9L+xuO8FoqTPXyIiN9xwQ4i7detmchUqVEj4d/R74t9z3cPn66+/Njl/7kXxSPf9rX493bNHRGTw4MFm+8Ybb0z4d/T9QLbe6+pzk+/dpb9nnnHGGSZXr169Ir+27w2lzyc+p3sjitjvMf7eW/et4vpbPH73u9+FONmzizhd/34OM54AAAAAAAAQCR48AQAAAAAAIBI5B/OxTu327dulYsWKxbE/gV8iMr/LAKd7qmtx2bZtW9Ip2vmRjnHNJPozWFzTUDNpXPUy0H369DE5vWS0X/rbl+pkg1SMq0jxjW25cuVCfNZZZ5ncwIEDQ9yyZUuTe/PNN0Psl/jWJVm+BCtTZdq4JqOngScrycqGa2wmj+tRR0XfQSGTPwPpvsYWdnwy+T0vDpl8zBYHXSIvIlK7du2EP6vL63zJb3ErCeOq73VFbPuB/v37m1yq9lWbOnVqiH0ps7+X+vbbb0O8e/fuIu9LVErCuCL18jOuzHgCAAAAAABAJHjwBAAAAAAAgEjw4AkAAAAAAACRiL4ZQCGxtCPSjc9gcnpZV9/HSfdk433MPHrZ+zFjxpjce++9F2LfCyguvZuyUbYu7Rs3HIMlG+ODdPC9mrKx12Zh6XtdEfvePfXUU5G/PucMxAkzngAAAAAAABAJHjwBAAAAAAAgEiW21A5A5qA0J3tQkgUAALIdZXBAwTDjCQAAAAAAAJHgwRMAAAAAAAAika8HTwcPHox6P1BAqRgTxrXkYVzjKVVjwtiWLIxrPDGu8cU1Np44ZuOJcY0nxjWe8jMe+XrwtGPHjiLvDFIrFWPCuJY8jGs8pWpMGNuShXGNJ8Y1vrjGxhPHbDwxrvHEuMZTfsYj52A+Hk8dOHBA1q1bJ7m5uZKTk5OSnUPhHDx4UHbs2CF16tSRI44oWqUk41pyMK7xlMpxFWFsSwrGNZ4Y1/jiGhtPHLPxxLjGE+MaTwUZ13w9eAIAAAAAAAAKiubiAAAAAAAAiAQPngAAAAAAABAJHjwBAAAAAAAgEjx4AgAAAAAAKCYHDx6U3/72t1KlShXJycmROXPmpHuXIpU1D566dOkiN910U7p3A8DPuPLKKyUnJ+eQ/y1dujTdu4ZC0mNaqlQpqVmzpnTr1k3+/ve/y4EDB9K9eyiC1atXy9VXXy116tSR0qVLS8OGDeXGG2+U7777Lt27hiLasGGD/OEPf5AmTZrI0UcfLfXr15e+ffvKpEmT0r1rKKT77rvvkGvrcccdl+7dQhFdeeWVct555x3y3/Py8iQnJ0e+//77Yt8npMbw4cOlbdu2UqFCBalQoYJ06NBBxo0bl+7dQoqMHz9eXnzxRRk7dqysX79e2rRpk+5ditRR6d4BAPB69OghL7zwgvlv1atXT9PeIBX+b0x/+ukn+fbbb2X8+PFy4403yltvvSXvvvuuHHUUl6NMs3z5cunQoYM0b95cXnvtNWncuLEsWLBAbrvtNhk3bpzMnDlTqlSpku7dRCGsXLlSzjjjDKlUqZI89thjcvzxx8v+/ftlwoQJMmjQIPnqq6/SvYsopNatW8uHH34Ytjn3AiVXvXr15JFHHpFmzZrJwYMHZcSIEdKvXz/54osvpHXr1unePRTRsmXLpHbt2nL66aene1eKRVZcba688kqZMmWKTJkyRYYNGyYiIitWrJBGjRqld8dQaF26dJG2bdtKmTJl5H//93+ldOnSct1118l9992X7l1DChx99NFSq1atdO8GUkiPad26deWkk06S9u3by9lnny0vvvii/PrXv07zHqKgBg0aJKVLl5YPPvhAjjnmGBERadCggZx44oly7LHHyl133SXDhw9P816iMH7/+99LTk6OfPrpp1KuXLnw31u3bi1XX311GvcMRXXUUUdxfQUyRN++fc32kCFDZPjw4TJz5kwePGW4K6+8UkaMGCEiIjk5OdKwYUNZuXJlencqYllRajds2DDp0KGD/OY3v5H169fL+vXrpX79+uneLRTRiBEjpFy5cjJr1iwZOnSoPPDAAzJx4sR07xaAfDrrrLOkXbt2MnLkyHTvCgpoy5YtMmHCBPn9738fHjr9n1q1asmAAQPkjTfekIMHD6ZpD1FYW7ZskfHjx8ugQYPMQ6f/U6lSpeLfKaTMkiVLpE6dOtKkSRMZMGCArFq1Kt27BCAffvrpJ3n99ddl165d0qFDh3TvDopo2LBh8sADD0i9evVk/fr18tlnn6V7lyKXFTOeKlasKKVLl5ayZcvy//LESNu2bWXw4MEiItKsWTN5+umnZdKkSdKtW7c07xmKauzYsVK+fPmw3bNnT/nnP/+Zxj1CVI477jj58ssv070bKKAlS5bIwYMHpWXLlofNt2zZUrZu3SqbNm2SGjVqFPPeoSiWLl0qBw8epPdPDJ122mny4osvSosWLWT9+vVy//33S8eOHWX+/PmSm5ub7t1DEfj7JpF/P6hA5ps3b5506NBB9uzZI+XLl5dRo0ZJq1at0r1bKKKKFStKbm6uHHnkkVnzfCIrHjwhntq2bWu2a9euLRs3bkzT3iCVunbtakp0Dvf/uiMeDh48KDk5OeneDRQSM5rihzGNr549e4a4bdu2ctppp0nDhg3lzTfflGuuuSaNe4ai8vdNIiKzZs2Syy67LE17hFRp0aKFzJkzR7Zt2yZvvfWWDBw4UKZMmcLDJ2QcHjwhY5UqVcps5+TksEJWTJQrV06aNm2a7t1AMVi0aJE0btw43buBAmratKnk5OTIokWLpH///ofkFy1aJJUrV2ZRgAzUrFkzycnJoYF4FqhUqZI0b96cVWNj4HD3TWvWrEnT3iCVSpcuHcb25JNPls8++0yGDRsmzz//fJr3DCiYrOjxJPLvg5YppwBQcnz00Ucyb948ueCCC9K9KyigqlWrSrdu3eTZZ5+V3bt3m9yGDRvklVdekYsvvpjZbBmoSpUq0r17d3nmmWdk165dh+RZmj0+du7cGVZVApAZDhw4IHv37k33bgAFljUPnho1aiSzZs2SlStXyubNm5kZAwDFaO/evbJhwwZZu3atzJ49Wx5++GHp16+f9OnTR6644op07x4K4emnn5a9e/dK9+7dZerUqbJ69WoZP368dOvWTerWrStDhgxJ9y6ikJ555hn56aef5NRTT5W3335blixZIosWLZL/+Z//oaltBrv11ltlypQpsnLlSvnkk0+kf//+cuSRR8qvfvWrdO8agMO48847ZerUqbJy5UqZN2+e3HnnnZKXlycDBgxI964BBZY1pXa33nqrDBw4UFq1aiW7d++WFStWSKNGjdK9WwCQFcaPHy+1a9eWo446SipXrizt2rWT//mf/5GBAwfKEUdkzf8HEivNmjWTzz//XAYPHiwXXXSRbNmyRWrVqiXnnXeeDB48WKpUqZLuXUQhNWnSRGbPni1DhgyRW265RdavXy/Vq1eXk08++ZA+Msgca9askV/96lfy3XffSfXq1eXMM8+UmTNnUhILlFAbN26UK664QtavXy8VK1aUtm3byoQJE1hICRkp5yBdJAEAAAAAABAB/m9mAAAAAAAARIIHTwAAAAAAAIgED54AAAAAAAAQCR48AQAAAAAAIBI8eAIAAAAAAEAkePAEAAAAAACASPDgCQAAAAAAAJHgwRMAAAAAAAAiwYMnAAAAAAAARIIHTwAAAAAAAIgED54AAAAAAAAQCR48AQAAAAAAIBI8eAIAAAAAAEAkePAEAAAAAACASPDgCQAAAAAAAJHgwRMAAAAAAAAiwYMnAAAAAAAARIIHTwAAAAAAAIjEUfn5oQMHDsi6deskNzdXcnJyot4nJHHw4EHZsWOH1KlTR444omjPDRnXkoNxjadUjqsIY1tSMK7xxLjGF9fYeOKYjSfGNZ4Y13gqyLjm68HTunXrpH79+inZOaTG6tWrpV69ekX6G4xrycO4xlMqxlWEsS1pGNd4Ylzji2tsPHHMxhPjGk+MazzlZ1zz9bgxNzc3JTuE1EnFmDCuJQ/jGk+pGhPGtmRhXOOJcY0vrrHxxDEbT4xrPDGu8ZSf8cjXgyemr5U8qRgTxrXkYVzjKVVjwtiWLIxrPDGu8cU1Np44ZuOJcY0nxjWe8jMe+Sq1A+JK16L6utQDBw4cNkbJcNRRiU9fP/74YzHuCQrqyCOPNNsHDx402xxvQPwkO2d7nMNLloKMnebP5ZzbUyfZPSsAJPuOqxXn9ZZV7QAAAAAAABAJHjwBAAAAAAAgEpGX2iWb2sW0UEShTJkyZrtWrVohPvroo02uVatWIW7durXJzZ07N8RjxoxJ5S6iEGrUqGG2k43JQw89lK+fQ2rp870/1mrXrh3ic8891+S++eYbs71w4cIQ//TTTya3bdu2EG/ZssXk/M8CKBxdDlulShWTq1ixYr7/jv7ZXr16mZw+X2zfvt3k3nvvvRD74/y7774LsS/The2zUbVqVZOrVKlSvv+OHrtOnTqZXIUKFRL+nr63nzNnjslNnDgxxHv27Mn3vuDf2rRpE+I//elPJvfXv/41xJMnTy62fQIQLV3qXKpUKZOrWbNmiJN9x23btq3J6WvulClTTE7fk+vrbSow4wkAAAAAAACR4METAAAAAAAAIsGDJwAAAAAAAEQikh5Puh+ArkcWEcnNzQ3xggULTE7XFFK3j2R0vauuYRUR6d+/v9nu06dPiH1fAt3vwPet0L0J3n//fZOjl0z0dI8REZGOHTua7ZNOOinEvl/cCSecEGJ6PKWWHhffP+SMM84I8emnn25yXbt2DXGLFi1M7ocffjDb33//fYj92M6bNy/Eb7zxhslNmzYtxBs3bjzs/gPZyvc/bN68eYj9dfP4448P8ZlnnmlylStXNtu6p1Ay+rr9cx555JEQ+x5w999/f4hfffVVk8uWe0c9lrqPpYhItWrVQjxo0CCTa9++fYiT9WAVsb1EdB8Rn/P0GPixGzJkSIjHjRtncps2bTrs38hm/pjp1q1biP0xu2jRohDT4ylz+eNSb/v7IXolx0Oya7OIyNlnnx3iunXrmlznzp1DnOw7rr9u79+/P8Rr1641uRdeeCHETzzxhMkVtTcfM54AAAAAAAAQCR48AQAAAAAAIBKRlNrpqV5+ifoTTzzxsD8nIrJ+/foQ7927N4pdQwnmpxTrqdzNmjUzOT3d+PLLLzc5P0VRLy+ZrCTgxx9/NNurVq0KMdO+i1/jxo3N9p133mm2dcmXn87vl3BGwSQrp9Mlj5dccknCnF+CPVmZTbly5cx29erVE/5s06ZNQ+yX+NaldrfddpvJrVy5MuHfRHL6XOyneWt+uraeyh0VfX6vXbt2wp9Lx76VBPo4vOaaa0zu4osvDrFvi6CPV1/6kazcw18rddmsjkXs58qXi+nSg2OPPdbkBg8eHOIJEyaY3ObNmyUu9HtQp04dk9P3PbqdgIg9Z/tjwi+3nQr+86CX6fav/9BDD4XYfz/QZXjbtm1L5S5mLF/SqM+//n3fuXNnsexTttPnw58rV80vfb7V329ERNq1axfiuXPnmtyHH35otvP73dmfp/U9n/+elKwlif7epL/DF2RfsoV/X/V97u9+9zuT82W0+juwPycUpIQ90e/5a+z5558f4tdee83kinovzYwnAAAAAAAARIIHTwAAAAAAAIgED54AAAAAAAAQiUh6POm6wRo1apjcL3/5yxCfcsopJrdgwYIQf/311yZX1OX7CsrXwefm5oa4fPnyJqfrqr/77juTozeQ5WtcGzZsGGJf06pr2c855xyT0/WupUuXNrl9+/aZbd3/x/f10DXyepl2EZHnn3/+sD+H6JQtWzbEAwcONLlGjRqZbV1bPmrUKJP76KOPUr9zMebPd7pnSLI+Tr7/k+4T4Olj6OeOJ32e8H9Tb/vry3nnnRdifzwPHTo0xMV9PSkqf97U77u/HkXRK7FFixYhHjZsWMKfu/HGG832/PnzU/L6mv+s6l4EN998s8lt3bo1xNdee63JZWPPL98DZuzYsSH29yq6B6c/XvW9mog91vzP6s+APyZ1v5C+ffua3B//+McQ+2u8vm/Q/UdEMrvHkz+f/va3vw3xBRdcYHKtWrUKsV+Ku7D8eUaPpe7bJGLH5IMPPjC5d999N8S+15++xs+ePdvkdu/eXcA9jr+aNWua7S5duoT422+/NbmpU6cWxy5lhWR9DXWvYt8bL789n/zxtGzZshDff//9JteyZcsQ696zIiL/9V//ZbZ9H71E9LVRxP6b/DlV92Lzfdn0v0P3bxMRee+990Kcrd+h9OdIn7NFRG666aYQ636LIgU7p+v31t/z+XOEpj/Xvm+Uvq8sbA+pRJjxBAAAAAAAgEjw4AkAAAAAAACRiKTULhk9fcwvs6qX8/PLMhZHaYQu4fBTK/X0wiZNmpjc8uXLQzxx4kST09PesrXsTr+vftnfxx57LMR+OUc97duPvy7F9NM7/fRt/VlKtoR2siWiUTDJSq701PtevXqZnJ56evzxx5ucH4833ngjxM8884zJ7dq1K9/7mi18GYUuU7v++utNTpe5+LHU5zFfWqyngU+ePNnkPvnkkxAvWrTI5HwZ5YABA0J87rnnmpwue/b/Jr2vfuryq6++GuKlS5dKJvElOHrZcb3UsojIk08+GeKRI0eanC9DTsRPrdbLO7dv3z7h73Xv3t1sf/XVV2Zbl8cWlr9v0OV1J598ssnpcjpf8pMtpXZbtmwJ8fDhwxP+nL+OJhPFtdKXCNx+++0Jf/ann34KcSo+U+mkz2H+PHjVVVeFONn9kX4/ROyY+9KbZCVEvtxCn999SY82evRos63bG/hztMY9189r3ry52dbnP186u23btmLZpzjyJdy6FYw/N+qSKV+WluzzrvnvIvqY9S0EdFmrPw/oFgIi+T+G/L2Avj7684AuH/SlhPrfccIJJ5jcuHHjCrxfma5atWpmW7d/ePDBB01Ol9H65wMrVqww2++8806I/Tldl7PPnTvX5PT3X1/O/tRTT4X4mGOOMbnXXnstxGvWrJFUYsYTAAAAAAAAIsGDJwAAAAAAAESCB08AAAAAAACIROQ9nnxdp64P9X0rTj/99BDrGnERkU2bNhV5X3xtaoMGDcx227ZtQ6xr60Vsvxm/vKmuoRw8eLDJzZgxI8TZ0lPC0+N82WWXmVzjxo0T/p6ucX355ZdNTvcu8Us7+34HyD9/jOi6d/+5131gfB+gzp07h9jXsvfs2TPEemlgEVtn7sfVL9ms+zr58wUOVb16dbOt6811HbqIHU8/fgsXLgyx7ickIjJt2rQQ+z59uheL7+115ZVXmu1zzjknxOXKlZNEfF283p41a5bJ5XeZ4ZLI95Ho1KlTiH3PB91nafz48SaX3x5Pvk+Ffv1k/R86dOhgcv68vXHjxny9fkHofU3Wx0y/ZyK2FwLXjPT0StJ9JXRPFRHb18TTS3hnej+/OnXqhNj3UdL97L744guTGzNmTIi//PJLk5s+fXqI/bLp+nyh+4aIHNpPSP+ufj2RzOuTl6nOOOMMs125cuUQ6+8XIvR4Kih97frFL35hcvfff3+IW7RoYXL+GliY1/M9dXy/NU1fn/x36ooVKxZqX/x93bfffpswl4y+bvhzTbb0Nda9q3/zm9+Y3A033BBi37tLvz/+HP7nP//ZbOtzte+HqD8TyXpp+f6Yeszffvttk3vppZdCnOoe28x4AgAAAAAAQCR48AQAAAAAAIBIRFJqp6d76mX+RGy5g1/euGvXriFet26dyelpaMmmhPuSn/r164fYlyT06dPHbOvyoJYtW5qcniLpX6NJkyYh1ks7i4jUqlUrxHrpQpHMXwY4v/RU4Y4dO5qcfi/98pH33ntviEeNGmVyP/zwQyp3Mavpz7Y+BkREevfuHWJfFqfLADxdluc/57p0SpdMiNhygl//+tcm9/XXX5vt3bt3J3x9/Js+vvyxd+6554bYLwGrpwDr0joRkeuvvz7En3/+ucnpUi4/lVyX1z3++OMm17Bhw4T7XRC6nHnIkCEmp5cGz3TJljeuUKFCiAtbEqDLOURE2rRpk/D19PaZZ55pcr5MRC+7XtjllfX0cBGRKVOmhLhdu3Ymp68Tvhw3W8oA0k2XP+p2BiIit956a4h9ua/+XPlyuuHDh4d48+bNqdjNtNH7P2zYMJPTx68vxVi9enWIfalosmNLv54v1fLnYaSHbmPgy9J1+wPfjiCTy8nTQZ9j9DVOxN6/PPfccybn71vzS5cTd+vWzeR0abEvpZo4cWKIfcltYa+j/t8wderUEBe2ZHPLli1mO1tK2HW59AUXXGByvkWJpr/z+nYTX331ldn2n4nC8N+j58yZE+LPPvvM5KL8js2MJwAAAAAAAESCB08AAAAAAACIBA+eAAAAAAAAEIlIejzt3LkzxKtWrTI5Xavv+0jo3i/ly5fP9+vpOni/tKTua3Lqqaea3PHHH2+29ZKIfr+1qlWrmm3d68b3vSnIvyOudO20H3NdA+yX9tXb9HSKToMGDUI8dOhQk9M9OdauXWtyup47Ly/P5JL1edM9g3xNuF6ONVt6oEVJn6suueSShDlP90ryy7rqvk66p5OI7REycOBAk7v88stDrPviHY7uC5OsF4/v/aKXgF2+fLnJZXJPH38s6ONLv1ciIq1btw6xP99u2rQp4Wvofhenn366yelt3+NJv75/Pd/TZ+zYsSEubG8KL9m46p4nvldZql4flv886mP9xRdfNLnjjjsuxLp3jYg9tv29wYgRI0Kcyce1iO3dMW7cuIQ/l6rPq+519+STT5rciSeeaLZ1n5vC9t1D0eiefSL2+PKfiUw/Foqbfv+mT59ucrqnmr+/LWy/HX0P5Psf6h5P/l77gQceCPHcuXNNLlVjzv12/vlrlX7OoL9Pef49HjlyZIh9D78ojmX/XEP3CSzOcwczngAAAAAAABAJHjwBAAAAAAAgEpGU2u3fvz/EfllGnfOSLRGt+ancempbnz59TE4v1+unkvlp33qq27x58xK+pp4u6V/Tl9bp0j+/tHW2TG3UY+nHTo+JX94zFctH4ufppXv9tG495ffaa681uSVLloTYL3Guj3M/HZwSl+j486aezq2nA4skL514+eWXQ/zuu+8mfI2CLI+uS6m9ZOcFb/fu3SGOcwmO5peM18viNmvWzOT0++yXR9fHrH9/9Bj484AuA0j2vvrPX6NGjRLuW2GX/vbX2MaNG4fYf4527NgRYq4n0dFtClq0aGFy+pzgzxd6vPbs2WNyusRXH9ciIt98803hd7YEK45roz5+v/76a5P78MMPzfall14a4vbt25ucLsfm2Eotfx5DNPTx9vHHH+fr54qiXr16Ifb3Q/o19DL3IvZ8l+w7NIpH7dq1zfagQYNCnKyFhS91mzp1aojTcb+arntkZjwBAAAAAAAgEjx4AgAAAAAAQCR48AQAAAAAAIBIRNLjKQq6d4TvTVK/fv0QJ+spoWvSRUQmTJhgtpctWxZiveSsf/0vvvjC5HRPA79Ept4f35tC18XHue9Nbm5uiAvSywXFQ/ca0z1RRGz/tLp165qcXmK2IH1fkvVvK8n0MVpSj9dkPXb0cej99NNPZvuHH34I8d///neT00tu16lTx+R0r5dkn4mCnAdWrFhhtu+9994Q6+VoRWz/pzipVq2a2e7evXuIy5Yta3JbtmwJ8aZNm0wu2fusexAee+yxJqd7PC1YsMDkdN8Kf43z19FU9ILxvTF0jyf/79PnKN+HDoXn77N0zzHf40mfEzx93nnzzTdNbvjw4SEuyOcY+ef7xfjrf5UqVUKs+5iIiEyZMiXE/t4aRaPvtfT51Sup9yGZqDjey2T9bvXr+2tsYfshIhq6L66I7eXsx1V/v3r11VdNbvz48RHsXcmXmd8AAQAAAAAAUOLx4AkAAAAAAACRKFGldnqKmi+na9KkSYhPPvlkk7vllltC7JcyHDt2bIjHjRtncp9//rnZzm8ZwLx588z28ccfH+KePXuaXJs2bUJcuXJlk9PTJ+M0ZdZPQ+zSpUuI/bjq0pjt27dHul8/x+93pUqVQuynoMdp+WBdgqJLU0REbr755hDfc889JqdLXOLy+dX/Dj1FVkRk2rRpIf7kk08S/l46+X3W5RDr1683OX1O9cflnXfeGWJfPuV/Vtu1a1eI/ZLArVq1CrEu4TgcvbT6yy+/bHK6rCeupXXe0Ucfbbb1cr66RE7EfgZ27tyZ79fQZZO9e/dO+HP+s3/OOeeEuEKFCibnS7L0Z6mwY+fP0/7zqenzNstQF55/jwcOHGi227ZtG+JkS8H7nD6X9OnTx+R0Od2TTz5pcgsXLgwx4xodXRrky331OZxSu9TS53t/ftflqfPnzze5knIfgqLx40hpcebS51B9Dyxiv2P6cvI4Y8YTAAAAAAAAIsGDJwAAAAAAAESCB08AAAAAAACIRInq8aR7N/h+SGeeeWaIO3fubHINGzYMsV++ecKECSH+9NNPTW7fvn2F2k+/9Liux/U9DHR9tu+ToP+9cepT4Pu31K9fP+HP6qW/v/jiC5PT76vv65GMr4mvWbNmwr+jl8HUvahERE477bQQjxgxwuR0v7BMr6vX/aqmT59ucjfeeGOIdU8gn/Pvud/WdL362rVrTU4fB8mWERaxvan86/3www8Jf0/3EtM94EREvvnmmxB/+eWXJrdq1aoQZ8qY632eOnWqyek+Qcccc4zJ5ebmhtif7/Qxu3r1apPT76fuwyIi8uijj4bYn999D4PFixeHeOTIkSaXbGzjRPcGaNmypcnp3gCp0q5duxD7c7buP/DOO++YXPXq1UPsezqdcMIJZlv/3cL2NEjWYwyF5/uItWjRIsTnn3++yV1xxRVmO1lfJy1ZrxJ/33DJJZeEWN/jiYhcffXVIaa/UPHQ91Ei9j589uzZxb07sVKmTBmzrT/7/nyne8z6vrWZcl+SrfR9lVfY3owofr6XarLx0vdxuueziO1JXZD7If03D7ediN/vdGHGEwAAAAAAACLBgycAAAAAAABEokSV2tWtWzfEv/rVr0xOTz0tV66cyemyDF8GkGzp7SiWqPTTYmvVqhViXyKoS1Yyfbq4nup3xhlnmJyeTuin5Otct27dTE6X1Pi/6Zft1vx0Vl1C53O6REuXjIjYacu+1C5O9L/Tl9rp0jNfajd69OgQL1iwwOR86Vai13v//fdNTk9Z7dWrV7LdNqVjvkxDl5j5UjE93dSX+ul987+XiXTp8ZAhQxL+XIcOHcy2Hk89tV/EliDOnTvX5PR7dv/995ucPhd6fpqxXj5dn9+zif4s6s+ziMiuXbtC7M9b+eWnZw8YMCDE+rwskvxapT8Pffv2NTlfste7d+/D/p5I8mngukTanxd+riQX+XPdddeZ7QceeCDEvk2AHyt9n1WQJd71Z7BTp04mp++X/PnpnnvuCfFvfvObfL8ekku2jLsvZ092D4aC8dfGPn36hNjfh3z44Ych3rNnT8K/+XPtKUpK2U2c+THQ1z+f099Pd+zYYXJRfFfFoWOgWxj476r6uUOdOnVMTrfv8GOl/061atVM7vLLLw/xSy+9ZHL+uNfPFlq1amVyuoTP39fpc/rLL79scul67sCMJwAAAAAAAESCB08AAAAAAACIBA+eAAAAAAAAEIkS1eNJ15D75aJ13eS+fftMbs6cOYeNRexS8cVRJ+vrQnUNqe+T8HM12JlEL8V8+umnm5yuOfVjp3vQ+CWadZ8vv5Sv7zeQjK6N9XWzW7duDbHvJfPuu++GeNKkSSYX1z4SGzduNNuvvPJKiO+8806Ta926dYgHDx5scqmoHc7W3j6ppM95y5cvN7nbb789xP58u23bthB///33JqePIb8E+0UXXRTiHj16mJz+Wd+bwi8Lrbf1OTxb6R4CIiL79+8v8t/0/Qj9Ur+a7vmlPxsi9lyYrL+BiP2c+c9Osp4j+u/4z6q+FvjXj+t5OgrLli0z21999VWI/Xvue+MNGjQoxPqaLpL8vkvfA+m+ZSK2x5O/3idblhz5548P359Ln/srV65cHLuUlfx3Ad0/yx8/+vzrz6GnnHJKiP/yl7+YnD++dG80fayLcM2Niv4u5Md1xYoVIZ45c6bJxaHfaEnRoEGDEF9wwQUmp3sJ+l5JjRo1CrHvKVu7du0Q+3sezf/ezTffHOILL7wwyV5b/nqcrDdVsuvvY489FuJk/eJSjRlPAAAAAAAAiAQPngAAAAAAABAJHjwBAAAAAAAgEpE0GdI1hr6PRH5/z9dX6t4zM2bMMLkhQ4aEeM2aNSYXRW2s/zf5fdWS/ZviRPdg6tq1a8Kf8+Nzxx13hLh3794m1759+4S/VxA7duwI8eTJk03uk08+CfHcuXNNbt26dSEuzvrXkkT3XfG1wrrPRpz6lcWVH7/NmzcfNv45+vz3i1/8wuTuuuuuEFevXj3h35gwYYLZ1udwEZFNmzble3+yUWF7F+nrUcOGDU1O9x/YvXu3yelr7pYtW0xO94XRPfNERGrUqGG2+/XrF+KpU6ea3NixY0Psr9v169cP8QknnGBy+rrqP8d63+j3lNzEiRPNtn7v/Pnd9+PasGFDoV5T927ynzl9vvJjt2jRooQ55J9/7xYuXGi26fFUPJJ9T/I99fRnv3v37iZ3//33h7hly5Ym56//L7zwQojvu+8+k9P9TVF4vjed3vbXuA8//DDES5YsiXbHsojvedS4ceMQX3nllSbXvHnzhH+ndOnSCf9mYfelTJkyIW7WrJnJ+eNVbxfkWYI+x/t7Pv369HgCAAAAAABAxuPBEwAAAAAAACIRSX2MnpZdvnx5k/NTD7Vky/7pqdzJSqKKYxlQ/28qV65c5K9Z0vhxTLbUpJ7C99JLL5nce++9F+KRI0eaXBSlicmW7E7Glxroz2qclzp9//33Q3zVVVelcU+QLtWqVTPb1157bYivueYak9PnAU8vF6yXkRURWblyZeF3MAvokhcRkVmzZoVYTx0XsefmihUrmpxedveGG24wOV1KM2XKFJMbM2ZMiP01dvr06SEeN26cyV1yySVmW0/19tPc582bF+JvvvnG5M4555wQd+rUSRKZM2eO2f70009DTElWcn5cozgmjz32WLOtS3zOP//8hL/n9+Xzzz9P5W4Vq4Isdx01vy+6fF4k+f06UqdXr15mu169eiH2Y9S5c+eEv6evBf585z9numT5ySefNDndnsJfCziP5p9uQXK4bU2/5/v3749sn7KN/9zrEv+zzz7b5PT9kS9/1cdahQoVTE5/V/WtAHQ7iqOPPjrhfurvwn4/RWy7oQYNGiR8fX986pJ5/zf9fWVxYcYTAAAAAAAAIsGDJwAAAAAAAESCB08AAAAAAACIRCQ9nnQdq+/HoHPJlmD3S/uNGjXqsLGIyA8//FCo/SwIva/+39SlS5cQ+7rQuPb/qVu3rtm+9957E+bWrl0bYr1ktsihSyhr6a4lL1u2bIh1XxsRW4P96quvmpxfbjyT7dy5M8TUnWePY445JsS9e/c2ueuvvz7EyXoW6M+OiO3v5nv4ILmtW7eabd1349xzzzU5PSZnnXWWyek+Pq1atUr4eqtXrzbbur+At2nTphA/88wzJtexY0ez3aRJkxDrvk0iIpdffnmIhw4danLJeiPoHg66F6SIyK5duxL+HqLhezPqc4lftl33ddI/JyKybNmyEOtl4kVEJk6cWNTdLFa6f5pf5n727NkhLs4lrUVEqlatarYHDRpktmvXrh1i3ysl3fdnceJ7xujeWv570hVXXBFi329W36P5e21/zb300ktD7HvG9OzZM8QzZ840uWT37LD82NEzLf30eWzz5s0m57c1fT3ydB+2du3amdzxxx8fYt8DVZ9DR4wYYXLvvvuu2dbPEnzft2T0a5SUczYzngAAAAAAABAJHjwBAAAAAAAgEpGU2unpheXKlUuY8/RUsu3bt5ucXhLQT6cvbn56q/43Jlsqt6RMc0sFP466vM5PJ9XTf7dt2xbtjhVBsmXj77jjDpNbuHBhiP2U5jiV2uklRPUSvyIiq1atKu7dQUT8eUuXwNx9990mV6NGjYR/R09Vfv75501OTyUuyhLiel/TuRR5cfLjo8+3/hqry719maQuEz/99NNNbsWKFSF+5513TE4v9ezpMfDL3vvle+vUqZNwv3UJyZw5c0zummuuCbEvS9Gl9vPmzTM5X+6J/NMlc758zl//dalO69atTU5/zvR5RUSkTJkyIfZlkbosb+TIkSZX3CVpRaVL2u666y6T06X6/t8ZRVmTvj/zY+XbSOgSV11SK2LvyVE0/vjS/Llfn//8cfD666+H+De/+Y3J/fjjj2b72WefDfHjjz9ucvpcPGvWLJN7++23E+4rEFf++EnE3ysla1GinwksXbrU5Pbt21eAvcsszHgCAAAAAABAJHjwBAAAAAAAgEjw4AkAAAAAAACRiKTH0/fffx/iuXPnmpxe0lP3qRCx/Rg+/vhjk5s2bVqIv/vuu1TsZqH5emy97fs46ffC18SX5H5HqaR7ClSqVMnk0t2vJb/LxuteFCL28/jtt99GtHfpp8fL9+5asGBBiLPlsxwn+tjTy9yL2P4qPqd/z5/vpk+fHuK//OUvJqePE7+sbLLef7pnkYjtJ+P7jiTr6aNr9NevX29ye/fuTfh7JYF/n/Wxt3XrVpPTPbjOPPPMfP/NIUOGhHjChAkmp/svJuOvzQ8//LDZ1svK+/Ot/kw8/fTTJqf/TX6/P/zwwxCPGjXK5PLblyHO9PGqew2J2GOpb9++JpdsGWjfg/OMM84IsT9e9XX9/fffNzndA2zixIkmp/t1ZXovN/05rF69uskNHjw44e/pz7PuZfZz9D2pH49WrVqF+KGHHjI538dRW716tdn29/YomCpVqoRYHz8iyXs+af469swzz4T45859uh+f78Wne3tedtllJqd7mpb062a6+WMP8aTvX/V1U+TQ77ya7ge8du3alO9XScWMJwAAAAAAAESCB08AAAAAAACIRCSldnq6vZ8+racVd+vWzeT0Ust+6qeekpbfaf9R8VP99XayUruFCxcmzGUaP43XT73XateuHeIbb7zR5P785z+H+KuvvjK5VE3j1dMg9fRmEZGePXuGONmy8cuWLTM5/fmM03RjP8VbL7fsc7oUIpM/y9lCL40tInLccceF+NZbbzU5XV7nl3P225oupfLLOetyGT2VX8QuEf1zypYtG2K/5Gyy8gJdDvrEE0+YnC5pKYlLtScraRw3bpzJXXjhhSH2Y67PVbpETUTkvffeC3Fh3wNfErVixQqz7UvotHPOOSfEvgxfX/P933zxxRdDrEv548yXPZcrVy5hrmHDhiEeNGiQyXXs2DHE9evXNzldJuLP/X6c9fhs3rzZ5HRZ+h133GFyq1atCnGcyyJ1mdqjjz5qcro86n//939NTp+Xn3zySZPT5wBfzqHHuUOHDian74F86aU/t2/YsCHE//Vf/2Vy2XKsRUWPQ6dOnUwuWamdPvZ8abM/9pLRx9vLL79scrfddluI/bVanyf8EvCwY6fLWn3Of29I9h0KJYu/r+rRo0eIr732WpPT51j/7EJfG/UzjrhjxhMAAAAAAAAiwYMnAAAAAAAARIIHTwAAAAAAAIhEJD2edA2yr2OdMWNGiP3S13opV98Pyfe4SCe/33o7Wf8nX9+ZyUsE+6V1H3zwwRDrpdhF7PKSAwYMMLnzzjsvxB999JHJzZkzJ8S+/nn58uUh9kuWnnbaaWa7a9euIfZ9LHSdve8X8+6774b45ptvNjnd3yCTx9GrVq2a2dY9QPz7M378+BD7niy6r5ZXsWLFEPv+AboGXi8ZL3Lo8aNfc/To0SbnP5/ZSi+Dro9REXvs6R4xP0d/3n0vCt2rwvetKIhkffO0ZJ+zZHRfFRH773/hhRdMLt09BQ9n06ZNIb7//vtN7ssvvwyx752lz6P+mClIf5D88u9dXl5eiH2vJr1s98UXX2xy+lzwyiuvmNyECRMSvl6m0dcj349Qnzf9Eue6V6H/Pd3/pyDHi34vN27caHK6N4WI7f/jz70lqT9nuuh/t3/v9LY+J4uItG3bNsSPPfaYyW3dujXEvq+X7qvp+5Eks3LlSrP9t7/9LcRTpkwxuWwdy1Tx961asj6K2uuvv26216xZU6h98cf3P/7xjxBff/31Jqfv5/V9uEjJ+p6WLnrsGjRokDD37bffmpw+vuLc7y5T6eO1T58+JqfvJ3VvYBGR/fv3h/jTTz81OX3vlk3nU2Y8AQAAAAAAIBI8eAIAAAAAAEAkIim103wZkp7Kq5fSFbHTEP3vpXsKp576OHHiRJMrU6ZMiPW0OpHkS86n+99UFH5aoC538NO177nnnhDr0i0RWxbgy650+YCferpjx44Q+2nJfmlhPUXS77dejtYvSz5kyJAQ+39TnOhyqTPOOMPk9Pj88MMPJqfLPfySzbm5uQlfT5d71KpVK+HP+eVF9fLRIiKff/55iP2y0HrKeZxKIX+OL6Xp379/iH0ZR7LyOn28+fdPn8f8Oa2w/LlQl1b5kku93126dDE5/Xn15Z/63zR//nyTmzlzZogzYcqzHhN/bnrqqafy9TfSMZ1fv7e+TGPo0KEhfvXVV01u27ZtIfbnhUwYr/zSSzH7crpjjjkmxLotgUjhS071Z8C/r7oEzJf0+HIxX6qDxPQ9h4gta2rXrp3JHXvssSH2ZfB+O7/0udZf00eMGGG2hw0bFmLfYgIF48vSW7VqlTCn+euvPmYnT56cMFcU+nzr76/btGkT4jFjxphcJn+nSRV9PfLl7LrViC6VFTn0/IuSRX+n8d9V/fcPTX8Xufvuu01u8eLFKdq7zMKMJwAAAAAAAESCB08AAAAAAACIBA+eAAAAAAAAEInIezwlk6n1wBs2bDDbuj+Ir5lftmxZiHVfIpHM/fcfzt69e0Osl/MWERk0aFCIfQ+h008/PcS6H42ISP369UPse1hUrlw5xL6vll9SVtdO+5r4Tz75JMS+h5BesjzOdH8BXb8vYvtj+b5NV199dYj90rD68+B79Oi+Z2PHjjU53VvA8zXw1MQXjD/f6OPG9xvQfUf8mOj+SAsXLjS5VPXb0a/p+0jpc0HNmjVNTvd46t27t8npHiW+T1+cau0zdSnmPXv2hHjp0qVp3JPi45dU19fA4447zuTyu8S6P851T6HVq1ebXF5eXoj99U9v+75EceqrVdyS9ccsW7asyd1+++0hbt68ucmVKlUq4WvovkD+3P7xxx+HeMaMGSb39ttvm236OqWO7+PUtm3bhLlkfSn1ePrjOVXee++9EF955ZUmp89Z+T0nZatvvvnGbF900UUJf3bt2rVR7w6KQN+f+L6a+nv/7t27TW7SpEkh9t+F/HfXbMGMJwAAAAAAAESCB08AAAAAAACIRFpL7TLV5s2bzfaUKVNCPHXqVJPTU2bjVFpXEHqpZb+86AcffBBi/96dcMIJIU623Oz27dvNth4PEVuSlawkLFvHR/+758yZY3K6BGnXrl0m99prr4VYl2yI2FIpX6qlS6eydappVHyZ1ahRoxL+bPny5UOsy+dERMaPHx9iP0b685KOY0bvj5/yrLd9ya+WqeVoiBdfdvXggw+GWF//RJJfA3VJ1PLly01Ol8z58mSOg/TTJRwjR440OV2acfbZZ5tchQoVEv5NfV7253Y+D+nhy+d0+eqSJUvy/Xf0Nc6XUaaK/lz4smd93+BLQ5O1SshGfsz9/Qoyhz5Pv/zyyyanv4OuWrXK5JKVrGcrZjwBAAAAAAAgEjx4AgAAAAAAQCR48AQAAAAAAIBI5BxMtm7n/2/79u1SsWLF4tgf5NO2bduS1vjnR0kbV9/DIllPi2QyuU9BSRtXvXS9l8nvc3FLxbiKFG1sk42l79WUrf3OCqokjCtSL13jqq95hb3+cSwnV9KusfmV7PydTLZ8HjLtXFzYY11/ZfM94qJw5JFHJswVx+tn2rgif+I0rvoY8Y9U4nq+TSQ/48qMJwAAAAAAAESCB08AAAAAAACIROHm7gIRyJYp4ZmEcrr4YCyBkk1f87j+QeP8HS+ZcqwXRzkdkMk4RgqGGU8AAAAAAACIBA+eAAAAAAAAEIl8PXjKx8J3KGapGBPGteRhXOMpVWPC2JYsjGs8Ma7xxTU2njhm44lxjSfGNZ7yMx75evC0Y8eOIu8MUisVY8K4ljyMazylakwY25KFcY0nxjW+uMbGE8dsPDGu8cS4xlN+xiPnYD4eTx04cEDWrVsnubm5kpOTk5KdQ+EcPHhQduzYIXXq1JEjjihapSTjWnIwrvGUynEVYWxLCsY1nhjX+OIaG08cs/HEuMYT4xpPBRnXfD14AgAAAAAAAAqK5uIAAAAAAACIBA+eAAAAAAAAEAkePAEAAAAAACASPHgCAAAAAABAJGL/4KlLly5y0003pXs3AAAAACDl+L4DZJ6DBw/Kb3/7W6lSpYrk5OTInDlz0r1LkYr9gyfE05VXXik5OTmH/K9Hjx7p3jUUEWMLlGzPPfec5Obmyo8//hj+286dO6VUqVLSpUsX87N5eXmSk5Mjy5YtK+a9RFEMHz5c2rZtKxUqVJAKFSpIhw4dZNy4ceneLaTQlVdeKeedd166dwNAAoe7F9b/u++++9K9iyii8ePHy4svvihjx46V9evXS5s2bdK9S5E6Kt07ABRWjx495IUXXjD/7eijj07T3iCVGFug5Oratavs3LlTPv/8c2nfvr2IiEybNk1q1aols2bNkj179kiZMmVERGTy5MnSoEEDOfbYY9O5yyigevXqySOPPCLNmjWTgwcPyogRI6Rfv37yxRdfSOvWrdO9ewAQe+vXrw/xG2+8Iffee68sXrw4/Lfy5cunY7eQQsuWLZPatWvL6aefnu5dKRZZMePpwIEDcvvtt0uVKlWkVq1aPCGOiaOPPlpq1apl/le5cuV07xZSgLGNn127dskVV1wh5cuXl9q1a8vjjz9OaUCGatGihdSuXVvy8vLCf8vLy5N+/fpJ48aNZebMmea/d+3aNQ17iaLo27ev9OrVS5o1aybNmzeXIUOGSPny5c3YAihZ+L4TL/oeuGLFipKTk2P+Gw+eMtuVV14pf/jDH2TVqlWSk5MjjRo1SvcuRS4rHjyNGDFCypUrJ7NmzZKhQ4fKAw88IBMnTkz3bgFA1rjttttkypQpMnr0aPnggw8kLy9PZs+ene7dQiF17dpVJk+eHLYnT54sXbp0kc6dO4f/vnv3bpk1axYPnjLcTz/9JK+//rrs2rVLOnTokO7dAZAA33eAzDFs2DB54IEHpF69erJ+/Xr57LPP0r1LkcuKB09t27aVwYMHS7NmzeSKK66QU045RSZNmpTu3UIRjR07VsqXL2/+9/DDD6d7t5ACjG287Ny5U/72t7/Jn//8Zzn77LPl+OOPlxEjRpgeQcgsXbt2lenTp8uPP/4oO3bskC+++EI6d+4snTp1CjOhZsyYIXv37uXBU4aaN2+elC9fXo4++mi57rrrZNSoUdKqVat07xaABPi+A2SOihUrSm5urhx55JFSq1YtqV69erp3KXJZ0eOpbdu2Zrt27dqycePGNO0NUqVr164yfPhw89+qVKmSpr1BKjG28bJs2TLZt2+fnHbaaeG/ValSRVq0aJHGvUJRdOnSRXbt2iWfffaZbN26VZo3by7Vq1eXzp07y1VXXSV79uyRvLw8adKkiTRo0CDdu4tCaNGihcyZM0e2bdsmb731lgwcOFCmTJnCwyeghOL7DoCSLCsePJUqVcps5+TkyIEDB9K0N0iVcuXKSdOmTdO9G4gAYwuUbE2bNpV69erJ5MmTZevWrdK5c2cREalTp47Ur19fPvnkE5k8ebKcddZZad5TFFbp0qXDefjkk0+Wzz77TIYNGybPP/98mvcMwOHwfQdASZYVpXYAgPQ59thjpVSpUjJr1qzw37Zu3Spff/11GvcKRdW1a1fJy8uTvLw86dKlS/jvnTp1knHjxsmnn35KmV2MHDhwQPbu3Zvu3QAAABkoK2Y8IZ727t0rGzZsMP/tqKOOkmrVqqVpj5AqjG28lC9fXq655hq57bbbpGrVqlKjRg2566675Igj+P8+MlnXrl1l0KBBsn///jDjSUSkc+fOcv3118u+fft48JSh7rzzTunZs6c0aNBAduzYIa+++qrk5eXJhAkT0r1rAAAgA/HgCRlr/PjxUrt2bfPfWrRoIV999VWa9gipwtjGz2OPPSY7d+6Uvn37Sm5urtxyyy2ybdu2dO8WiqBr166ye/duOe6446RmzZrhv3fu3Fl27NghLVq0OOQ4RmbYuHGjXHHFFbJ+/XqpWLGitG3bViZMmCDdunVL964hRQ4cOCBHHcXXAABA8cg5ePDgwXTvBAAg+3Tp0kVOOOEEefLJJ9O9KwCQVXr06CFNmzaVp59+Ot27AgDIAtQ5AAAAAFlg69atMnbsWMnLy5Nzzjkn3bsDAMgSzLEFAAAAssDVV18tn332mdxyyy3Sr1+/dO8OACBLUGoHAAAAAACASFBqBwAAAAAAgEjw4AkAAAAAAACR4METAAAAAAAAIsGDJwAAAAAAAESCB08AAAAAAACIBA+eAAAAAAAAEAkePAEAAAAAACASPHgCAAAAAABAJP4/vTiznPY6910AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x500 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from torchvision.datasets import EMNIST\n",
    "\n",
    "# Get the EMNIST dataset\n",
    "# Normalize the image pixel values to within the range [0, 1]\n",
    "# Save the training data to variable X_train as a Numpy array\n",
    "# Save the testing data to variable X_test as a Numpy array\n",
    "# Save the targets to variable y as a Numpy array\n",
    "# Note that the images provided are inverted horizontally and\n",
    "# rotated 90 anti-clockwise. It must be oriented properly\n",
    "### --YOUR CODE HERE-- ###\n",
    "\n",
    "#Data Extraction\n",
    "dataset_path = '/home/pj/emnist_data'\n",
    "\n",
    "train = EMNIST(\n",
    "    root=dataset_path,\n",
    "    split = \"balanced\",\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "test = EMNIST(\n",
    "    root=dataset_path,\n",
    "    split = \"balanced\",\n",
    "    train=False,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "print(\"Train Len:\", len(train))\n",
    "print(\"Test Len:\", len(test))\n",
    "\n",
    "X_train, y_train, X_test, y_test = ([] for i in range(4))\n",
    "\n",
    "#Extract train\n",
    "for image, label in train:\n",
    "  X_train.append(np.array(image))\n",
    "  y_train.append(np.array(label))\n",
    "\n",
    "#Extract Test\n",
    "for image, label in test:\n",
    "  X_test.append(np.array(image))\n",
    "  y_test.append(np.array(label))\n",
    "\n",
    "#Fixing Dataset\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#Double checking the shapes\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"y_train:\",y_train.shape)\n",
    "print(\"X_test:\",X_test.shape)\n",
    "print(\"y_test:\",y_test.shape)\n",
    "\n",
    "#Rotating Function\n",
    "def fix_emnist_orientation(images):\n",
    "  fixed = []\n",
    "  for img in images:\n",
    "    rotated = np.rot90(img, k=1)\n",
    "    flipped = np.flipud(rotated)\n",
    "    fixed.append(flipped)\n",
    "  return np.array(fixed)\n",
    "\n",
    "X_train = fix_emnist_orientation(X_train)\n",
    "X_test = fix_emnist_orientation(X_test)\n",
    "\n",
    "#Get pixel value\n",
    "print(\"Min Pixel\", X_train.min())\n",
    "print(\"Max Pixel\", X_train.max())\n",
    "#Max is 255\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "print(\"Min Pixel\", X_train.min())\n",
    "print(\"Max Pixel\", X_train.max())\n",
    "\n",
    "# Display the first thirty (30) images from the training split\n",
    "# Place ten (10) images per row\n",
    "# Each image should have a label underneath\n",
    "### --YOUR CODE HERE-- ###\n",
    "\n",
    "#Get mapping for label\n",
    "mapping = [str(c) for c in train.classes]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i in range(30):\n",
    "  plt.subplot(3, 10, i+1)\n",
    "  plt.imshow(X_train[i], cmap='gray')\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.xlabel(mapping[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PabnczRfWZM3"
   },
   "source": [
    "2. Build the encoder subnetwork as shown below.\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=17C2wrfGKn6TPdrGezFWnSJG2jlCH_hmS\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "CzYXPQAgkDat",
    "outputId": "bc0504a9-56eb-498d-c9a2-37e882652981"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759044462.471841    5580 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1729 MB memory:  -> device: 0, name: NVIDIA GeForce MX330, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       "\n",
       " Conv2d_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,            <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>  Input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "                      <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                                              \n",
       "\n",
       " MaxPooling2d_0       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  Conv2d_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                                              \n",
       "\n",
       " Conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span>  MaxPooling2d_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "                      <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                                              \n",
       "\n",
       " MaxPooling2d_1       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  Conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                        \n",
       "\n",
       " flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  MaxPooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " Dense_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">200,768</span>  flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       "\n",
       " z_mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span>  Dense_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       "\n",
       " z_log_var (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span>  Dense_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
       "\n",
       " z_sample (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sampling</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  z_mean[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
       "                                                     z_log_var[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " Input (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)           \u001b[38;5;34m0\u001b[0m  -                 \n",
       "\n",
       " Conv2d_0 (\u001b[38;5;33mConv2D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,            \u001b[38;5;34m320\u001b[0m  Input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "                      \u001b[38;5;34m32\u001b[0m)                                              \n",
       "\n",
       " MaxPooling2d_0       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,              \u001b[38;5;34m0\u001b[0m  Conv2d_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling2D\u001b[0m)       \u001b[38;5;34m32\u001b[0m)                                              \n",
       "\n",
       " Conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,         \u001b[38;5;34m18,496\u001b[0m  MaxPooling2d_0[\u001b[38;5;34m0\u001b[0m \n",
       "                      \u001b[38;5;34m64\u001b[0m)                                              \n",
       "\n",
       " MaxPooling2d_1       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  Conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                        \n",
       "\n",
       " flatten (\u001b[38;5;33mFlatten\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m)                \u001b[38;5;34m0\u001b[0m  MaxPooling2d_1[\u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " Dense_0 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m200,768\u001b[0m  flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       "\n",
       " z_mean (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                 \u001b[38;5;34m520\u001b[0m  Dense_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       "\n",
       " z_log_var (\u001b[38;5;33mDense\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                 \u001b[38;5;34m520\u001b[0m  Dense_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
       "\n",
       " z_sample (\u001b[38;5;33mSampling\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  z_mean[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
       "                                                     z_log_var[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">220,624</span> (861.81 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m220,624\u001b[0m (861.81 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">220,624</span> (861.81 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m220,624\u001b[0m (861.81 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import functions and classes from Keras library\n",
    "from keras import Input, Model, ops\n",
    "from keras.layers import Layer, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "# Define the latent dimensions\n",
    "latent_dim = 8\n",
    "\n",
    "# Define the Sampling layer as a custom layer\n",
    "# The Sampling layer will accept two inputs\n",
    "# The first input is the distribution mean\n",
    "# The second input is the logarithm of the distribution variance\n",
    "# The layer samples from the distribution defined by the parameters above\n",
    "# Import functions and classes from Keras random module\n",
    "from keras.random import SeedGenerator, normal\n",
    "\n",
    "class Sampling(Layer):\n",
    "  def __init__(self, **kwargs): # initialize the custom layer\n",
    "    super().__init__(**kwargs)\n",
    "    self.seed_generator = SeedGenerator(0)\n",
    "\n",
    "  def call(self, inputs): # function definition when this layer is called\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch = ops.shape(z_mean)[0]\n",
    "    dim = ops.shape(z_mean)[1]\n",
    "    epsilon = normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "    return z_mean + ops.exp(0.5*z_log_var)*epsilon\n",
    "\n",
    "# Create the encoder network layers from the illustrated model plot\n",
    "# Use the latent_dim variable to define the probability distribution parameters\n",
    "# Save the input layer to input_img variable\n",
    "# Save the mean layer output to z_mean variable\n",
    "# Save the logarithm of the variance layer output to z_log_var variable\n",
    "# Save the sampling layer output to z_sample variable\n",
    "### --YOUR CODE HERE-- ###\n",
    "input_img = Input(shape=(28, 28, 1), dtype=\"float32\", name='Input')\n",
    "x = Conv2D(32, 3, activation=\"relu\", padding=\"same\", name='Conv2d_0')(input_img)\n",
    "x = MaxPooling2D(pool_size=2, padding=\"same\",  name='MaxPooling2d_0')(x)\n",
    "x = Conv2D(64, 3, activation=\"relu\", padding=\"same\",  name='Conv2d_1')(x)\n",
    "x = MaxPooling2D(pool_size=2, padding=\"same\",  name='MaxPooling2d_1')(x)\n",
    "x = Flatten( name='flatten')(x)\n",
    "x = Dense(64, activation='relu',  name='Dense_0')(x)\n",
    "z_mean = Dense(latent_dim, name=\"z_mean\", activation='linear')(x)\n",
    "z_log_var = Dense(latent_dim, name=\"z_log_var\", activation ='linear')(x)\n",
    "z_sample = Sampling(name=\"z_sample\")([z_mean, z_log_var])\n",
    "# Create the encoder network using the layers defined above\n",
    "vae_encoder = Model(input_img, [z_mean, z_log_var, z_sample], name=\"encoder\")\n",
    "vae_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1dNvMGoeHtb"
   },
   "source": [
    "3. Build the decoder subnetwork as shown below.\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1WaXSFDSDL_92iQQzrubMyYfYT7mlBzlz\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "ioDLrxE806CV",
    "outputId": "98aa48f7-6219-4713-ce09-8e27b0b3e939"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"decoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " Z_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " Decode_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">28,224</span> \n",
       "\n",
       " Flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " Conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> \n",
       "\n",
       " UpSample_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " Conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> \n",
       "\n",
       " UpSample_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">289</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " Z_input (\u001b[38;5;33mInputLayer\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                           \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " Decode_1 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m)                   \u001b[38;5;34m28,224\u001b[0m \n",
       "\n",
       " Flatten (\u001b[38;5;33mReshape\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " Conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m36,928\u001b[0m \n",
       "\n",
       " UpSample_0 (\u001b[38;5;33mUpSampling2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " Conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)             \u001b[38;5;34m18,464\u001b[0m \n",
       "\n",
       " UpSample_1 (\u001b[38;5;33mUpSampling2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " Output (\u001b[38;5;33mConv2D\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 \u001b[38;5;34m289\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,905</span> (327.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m83,905\u001b[0m (327.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,905</span> (327.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m83,905\u001b[0m (327.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import functions and classes from Keras library\n",
    "from keras.layers import Reshape, UpSampling2D\n",
    "\n",
    "# Create the decoder network layers from the illustrated model plot\n",
    "# Use the latent_dim variable to define the probability distribution parameters\n",
    "# Save the input layer to input_latent variable\n",
    "# Save the last convolutional layer output to decoded variable\n",
    "### --YOUR CODE HERE-- ###\n",
    "input_latent = Input(shape=(latent_dim,), dtype='float32',  name='Z_input')\n",
    "decoder = Dense(3136, activation='relu',  name='Decode_1')(input_latent)\n",
    "decoder = Reshape((7, 7, 64),  name='Flatten')(decoder)\n",
    "decoder = Conv2D(64, 3, activation='relu', padding=\"same\",  name='Conv2d_2')(decoder)\n",
    "decoder= UpSampling2D(size=(2,2),  name='UpSample_0')(decoder)\n",
    "decoder = Conv2D(32, 3, activation='relu', padding=\"same\", name='Conv2d_3')(decoder)\n",
    "decoder = UpSampling2D(size=(2,2),  name='UpSample_1')(decoder)\n",
    "decoded = Conv2D(1, 3, activation='sigmoid', padding='same',  name='Output')(decoder)\n",
    "\n",
    "# Create the decoder network using the layers defined above\n",
    "vae_decoder = Model(input_latent, decoded, name=\"decoder\")\n",
    "vae_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxHXZgEerRGL"
   },
   "source": [
    "4. Execute the code below that will define a custom model for Variational Autoencoder and instantiate its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GJq_J0xErlho"
   },
   "outputs": [],
   "source": [
    "# Import functions and classes from Keras library\n",
    "from keras.metrics import Mean\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "# Define VAE as a custom model\n",
    "class VAE(Model):\n",
    "  def __init__(self, encoder, decoder, **kwargs): # initialize the custom layer\n",
    "    super().__init__(**kwargs)\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "    # Define loss trackers\n",
    "    self.total_loss_tracker = Mean(name=\"total_loss\")\n",
    "    self.reconstruction_loss_tracker = Mean(name=\"reconstruction_loss\")\n",
    "    self.kl_loss_tracker = Mean(name=\"kl_loss\")\n",
    "\n",
    "  @property\n",
    "  def metrics(self):  # define model metrics\n",
    "    return [self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker\n",
    "            ]\n",
    "\n",
    "  def train_step(self, data): # define training process for the model\n",
    "    with tf.GradientTape() as tape:\n",
    "      # Forward pass: Encode the data and decode the latent representation.\n",
    "      z_mean, z_log_var, z = self.encoder(data)\n",
    "      reconstruction = self.decoder(z)\n",
    "\n",
    "      # Calculate training losses\n",
    "      reconstruction_loss = ops.mean(\n",
    "          ops.sum(binary_crossentropy(data, reconstruction), axis=(1, 2))\n",
    "          )\n",
    "      kl_loss = -0.5*(1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "      kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "      total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "    # Calculate gradients and apply update to model weights\n",
    "    grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "    # Update training losses\n",
    "    self.total_loss_tracker.update_state(total_loss)\n",
    "    self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "    self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "  def test_step(self, data):\n",
    "    # Forward pass: Encode and decode the data.\n",
    "    z_mean, z_log_var, z = self.encoder(data)\n",
    "    reconstruction = self.decoder(z)\n",
    "\n",
    "    # Calculate validation losses\n",
    "    reconstruction_loss = ops.mean(\n",
    "          ops.sum(binary_crossentropy(data, reconstruction), axis=(1, 2))\n",
    "          )\n",
    "    kl_loss = -0.5*(1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "    kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "    # Update validation losses\n",
    "    self.total_loss_tracker.update_state(total_loss)\n",
    "    self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "    self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# Instantiate the VAE model with its encoder and decoder subnetworks\n",
    "vae_cnn = VAE(vae_encoder, vae_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k36ALD1WAgOe"
   },
   "source": [
    "5. Configure the network for training with the appropriate loss function and an optimizer of your choice. Then train the model with a batch size of 100. Make sure to save the history of losses per training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SMH0HzsC0x4",
    "outputId": "2bfb0cb3-8b0a-4303-8cc4-3eaa2abd5dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 15:27:45.778391: W tensorflow/core/framework/op_kernel.cc:1855] OP_REQUIRES failed at xla_ops.cc:528 : INVALID_ARGUMENT: Trying to access resource Conv2d_0/kernel/0 (defined @ /home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/backend/tensorflow/core.py:42) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2025-09-28 15:27:45.778528: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Trying to access resource Conv2d_0/kernel/0 (defined @ /home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/backend/tensorflow/core.py:42) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.13/runpy.py\", line 198, in _run_module_as_main\n\n  File \"/usr/lib/python3.13/runpy.py\", line 88, in _run_code\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.13/asyncio/base_events.py\", line 683, in run_forever\n\n  File \"/usr/lib/python3.13/asyncio/base_events.py\", line 2042, in _run_once\n\n  File \"/usr/lib/python3.13/asyncio/events.py\", line 89, in _run\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_5580/4275259747.py\", line 18, in <module>\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nTrying to access resource Conv2d_0/kernel/0 (defined @ /home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/backend/tensorflow/core.py:42) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_3805]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Train the model. Set the batch size to 100\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Use early stopping to determine the appropriate training epochs\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Perform model hyperparameter tuning as needed\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Assign the output to hist_vae_cnn variable\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m### --YOUR CODE HERE-- ###\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.device(\u001b[33m\"\u001b[39m\u001b[33m/CPU:0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     hist_vae_cnn = \u001b[43mvae_cnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ml-gpu/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.13/runpy.py\", line 198, in _run_module_as_main\n\n  File \"/usr/lib/python3.13/runpy.py\", line 88, in _run_code\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.13/asyncio/base_events.py\", line 683, in run_forever\n\n  File \"/usr/lib/python3.13/asyncio/base_events.py\", line 2042, in _run_once\n\n  File \"/usr/lib/python3.13/asyncio/events.py\", line 89, in _run\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_5580/4275259747.py\", line 18, in <module>\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nTrying to access resource Conv2d_0/kernel/0 (defined @ /home/pj/venvs/ml-gpu/lib/python3.13/site-packages/keras/src/backend/tensorflow/core.py:42) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_3805]"
     ]
    }
   ],
   "source": [
    "# Configure the network for training using the compile method\n",
    "# Set the optimizer to your choice and determine the appropriate loss function\n",
    "### --YOUR CODE HERE-- ###\n",
    "from keras.layers import Activation\n",
    "vae_cnn.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model. Set the batch size to 100\n",
    "# Use early stopping to determine the appropriate training epochs\n",
    "# Perform model hyperparameter tuning as needed\n",
    "# Assign the output to hist_vae_cnn variable\n",
    "### --YOUR CODE HERE-- ###\n",
    "hist_vae_cnn = vae_cnn.fit(\n",
    "    X_train,\n",
    "    epochs = 50,\n",
    "    batch_size = 100,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, None)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzvO0ScbgSEJ"
   },
   "source": [
    "6. Plot both the training and validation losses per epoch for the following: (1) reconstruction_loss, (2) kl_loss, and (3) total_loss. Place appropriate plot title and axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "fr4eJ_3sv77N",
    "outputId": "20973d3e-4347-4e26-95b9-e582431ab53a"
   },
   "outputs": [],
   "source": [
    "# Extract the losses during training\n",
    "reconstruction_losses = hist_vae_cnn.history[\"reconstruction_loss\"]\n",
    "kl_losses = hist_vae_cnn.history[\"kl_loss\"]\n",
    "losses = hist_vae_cnn.history[\"total_loss\"]\n",
    "validation_reconstruction_losses = hist_vae_cnn.history[\"val_reconstruction_loss\"]\n",
    "validation_kl_losses = hist_vae_cnn.history[\"val_kl_loss\"]\n",
    "validation_losses = hist_vae_cnn.history[\"val_total_loss\"]\n",
    "epochs = range(1, len(losses) + 1)\n",
    "\n",
    "# Plot the history of training and validation losses\n",
    "# Put the training losses subplots in the first row\n",
    "# Put the validation losses subplots in the second row\n",
    "### --YOUR CODE HERE-- ###\n",
    "train_losses = [losses, reconstruction_losses, kl_losses]\n",
    "val_losses = [validation_losses, validation_reconstruction_losses, validation_kl_losses]\n",
    "titles = [\"Total Loss\", \"Reconstruction Loss\", \"KL Loss\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18,10))\n",
    "\n",
    "for i in range(3):\n",
    "  axes[0, i].plot(epochs, train_losses[i])\n",
    "  axes[0, i].set_title(f\"Validation {titles[i]}\")\n",
    "  axes[0, i].set_xlabel(\"Epoch\")\n",
    "  axes[0, i].set_ylabel(\"Loss\")\n",
    "  axes[0, i].grid(True)\n",
    "\n",
    "for i in range(3):\n",
    "  axes[1, i].plot(epochs, val_losses[i])\n",
    "  axes[1, i].set_title(f\"Validation {titles[i]}\")\n",
    "  axes[1, i].set_xlabel(\"Epoch\")\n",
    "  axes[1, i].set_ylabel(\"Loss\")\n",
    "  axes[1, i].grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pscQakiCw0rS"
   },
   "source": [
    "7. Obtain the output of the VAE for at least 15 test images. Display the input image in the first row and the reconstructed output in the second row. Compare the images and write your observations in the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "Tf5kj8-TYftg",
    "outputId": "638254f3-a115-41cb-80f4-7f144620d626"
   },
   "outputs": [],
   "source": [
    "# Select at least 15 test images\n",
    "# Feed the test images to the encoder network. Save it to z_img variable\n",
    "# Feed the encoder outputs to the decode network\n",
    "### --YOUR CODE HERE-- ###\n",
    "\n",
    "test_set = X_test[:15]\n",
    "z_mean2, z_log_var2, z_img = vae_encoder(test_set)\n",
    "#Reconstruction\n",
    "reconstructed = vae_decoder(z_img)\n",
    "\n",
    "# Display the selected test images in the first row\n",
    "# Display the resulting images in the second row\n",
    "### --YOUR CODE HERE-- ###\n",
    "mapping = [str(c) for c in test.classes]\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.title(\"Test Data VS Reconstructed Data\")\n",
    "plt.axis('off')\n",
    "for i in range(30):\n",
    "  plt.subplot(2, 15, i+1)\n",
    "  if i < 15:\n",
    "    plt.imshow(test_set[i], cmap='gray')\n",
    "  else:\n",
    "    plt.imshow(reconstructed[i-15], cmap='gray')\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.xlabel(mapping[y_test[i%15]])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXxtz6H6RWx8"
   },
   "source": [
    "8. Extract the encoder subnetwork and obtain its outputs for at least 1000 test images. With these outputs, plot the histogram for the all the latent variables. What distribution does the histograms show? Write your observations in the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "w0wIBHKfRXGl",
    "outputId": "11a97342-b6ce-4c1b-c758-54bcabe3930a"
   },
   "outputs": [],
   "source": [
    "# Select at least 1000 test images\n",
    "# Feed the test images to the encoder network. Save it to z_lat variable\n",
    "### --YOUR CODE HERE-- ###\n",
    "X_sample = X_test[:1000]\n",
    "z_lat, z_log_var, z_sample = vae_encoder(X_sample)\n",
    "\n",
    "# Create a histogram for each latent space variable into subplots\n",
    "# The subplots should be arrange into a grid with 2 rows and 4 columns\n",
    "### --YOUR CODE HERE-- ###\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(latent_dim):\n",
    "  plt.subplot(2, 4, i+1)\n",
    "  plt.hist(z_lat[:,i], bins=30, alpha=0.7)\n",
    "  plt.title(f\"Latent Variable {i+1}\")\n",
    "  plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOJFQKtwSVw_"
   },
   "source": [
    "9. Take the following three (3) images from the dataset: (1) a lowercase letter, (2) an uppercase letter of image 1, and (3) another uppercase letter. Obtain the encoder output for the images. Take the difference of image 1 encoder output and image 2 encoder output ($z_d = z_1 - z_2$). Afterwards, the difference shall be added to the encoder output for image 3 ($z_3'= z_3 + z_d$). Feed this as an input to the decoder network and display the resulting image. Can you make sense of the resulting image? Describe the effect of performing the latent space arithmetic and write your observations in the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "l9VjGeIhSVJ0",
    "outputId": "97c7fff3-9f5a-4333-a4e0-4de2acdbba84"
   },
   "outputs": [],
   "source": [
    "# Select the test image and display them in a single row\n",
    "### --YOUR CODE HERE-- ###\n",
    "\n",
    "#Selection and Index h 99, H 55, R 15\n",
    "indices = [99, 55, 15]\n",
    "sample_data = np.array([X_test[i] for i in indices]) #View the Images\n",
    "\n",
    "#Present the Data\n",
    "\n",
    "mapping = [str(c) for c in test.classes]\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "for i in range(3):\n",
    "  plt.subplot(1, 3, i+1)\n",
    "  plt.imshow(sample_data[i], cmap='gray')\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.xlabel(mapping[y_test[indices[i]]])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Obtain the output of the encoder for each test image\n",
    "# Save the encoder output for the first image to z1 variable\n",
    "# Save the encoder output for the second image to z2 variable\n",
    "# Save the encoder output for the third image to z3 variable\n",
    "# Perform the difference z1 - z2 and store to zd variable\n",
    "# Perform the sum z3 + zd and store to z3new variable\n",
    "### --YOUR CODE HERE-- ###\n",
    "\n",
    "z_mean, z_log_var, z = vae_encoder(sample_data)\n",
    "z1, z2, z3 = z_mean[0:1], z_mean[1:2], z_mean[2:3]\n",
    "\n",
    "zd = z1 - z2\n",
    "z3new = z3 + zd\n",
    "# Feed the new latent vector z3new to the decoder network\n",
    "# Display the resulting image\n",
    "### --YOUR CODE HERE-- ###\n",
    "final_image = vae_decoder.predict(z3new)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.title(\"Final Output\")\n",
    "plt.imshow(final_image[0], cmap='gray') #Not fixed at all\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-T0TkSE1FL1"
   },
   "source": [
    "**B. Generative Adversarial Networks**\n",
    "\n",
    "1. The code below converts the [EMNIST Dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.EMNIST.html) into a single Tensorflow Dataset format. Examine it then write the code below to display 30 images from the created Tensorflow Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "xf4dHz645ikl",
    "outputId": "0c7310b8-ebbf-4ba9-f181-6878c25cd9db"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import EMNIST\n",
    "\n",
    "# Load the EMNIST dataset\n",
    "emnist_train = EMNIST(\"./data\", split=\"balanced\", train=True, download=True,\n",
    "                      transform=transforms.ToTensor()\n",
    "                      )\n",
    "emnist_test = EMNIST(\"./data\", split=\"balanced\", train=False, download=True,\n",
    "                     transform=transforms.ToTensor()\n",
    "                     )\n",
    "\n",
    "# Define the generator function for Tensorflow dataset\n",
    "def emnist_ds_generator(dataset):\n",
    "  for image, _ in dataset:\n",
    "    yield np.transpose(image.numpy(), axes=(0, 2, 1)).reshape(28, 28, 1)\n",
    "\n",
    "# Define the Tensorflow dataset output signature\n",
    "emnist_ds_osig = tf.TensorSpec(shape=(28, 28, 1), dtype=tf.float32)\n",
    "\n",
    "# Create the Tensorflow dataset\n",
    "emnist_ds_train = tf.data.Dataset.from_generator(\n",
    "    lambda: emnist_ds_generator(emnist_train),\n",
    "    output_signature=emnist_ds_osig\n",
    ")\n",
    "emnist_ds_test = tf.data.Dataset.from_generator(\n",
    "    lambda: emnist_ds_generator(emnist_test),\n",
    "    output_signature=emnist_ds_osig\n",
    ")\n",
    "emnist_ds = emnist_ds_train.concatenate(emnist_ds_test)\n",
    "emnist_ds = emnist_ds_train.concatenate(emnist_ds_test)\n",
    "emnist_ds = emnist_ds.shuffle(buffer_size=1000)\n",
    "\n",
    "# Display 30 images from the Tensorflow dataset in three rows (10 images per row)\n",
    "### --YOUR CODE HERE-- ###\n",
    "\n",
    "iterator = emnist_ds.take(30).as_numpy_iterator()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i in range(30):\n",
    "  img = next(iterator)\n",
    "  plt.subplot(3, 10, i+1)\n",
    "  plt.imshow(img, cmap='gray')\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-4NPUawMu5U"
   },
   "source": [
    "2. Build the discriminator subnetwork as shown below.\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=17ZQvvO3HrEGoOMZliXsIF1xN8-nJeHoL\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "nA9jpo6XQq5F",
    "outputId": "8195c4b6-0d99-4013-b2c1-6efae2151f15"
   },
   "outputs": [],
   "source": [
    "# Import functions and classes from Keras library\n",
    "from keras import Input, Sequential\n",
    "from keras.layers import LeakyReLU, Dense, Dropout\n",
    "from keras.layers import Conv2D, Flatten\n",
    "\n",
    "# Create the discriminator. Save it to gan_discriminator variable\n",
    "# Convolutions should have a filter size of 5 and a stride of 2\n",
    "# Dropout rate is set to 0.3\n",
    "### --YOUR CODE HERE-- ###\n",
    "gan_discriminator = Sequential([\n",
    "    Conv2D(64,(5,5), strides=(2,2), padding='same', input_shape=(28, 28, 1), name=\"Conv2D\"),\n",
    "    LeakyReLU(name='LeakyReLu'),\n",
    "    Dropout(0.3, name='Dropout'),\n",
    "    Conv2D(128,(5,5), strides=(2,2), padding='same', input_shape=(28, 28, 1), name=\"Conv2D_1\"),\n",
    "    LeakyReLU(name='LeakyReLu_1'),\n",
    "    Dropout(0.3, name='Dropout_1'),\n",
    "    Flatten(name='Flatten'),\n",
    "    Dense(1, activation = 'linear', name='Dense')\n",
    "])\n",
    "\n",
    "gan_discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TuzvFQdQrTl"
   },
   "source": [
    "3. Build the generator subnetwork as shown below.\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1zavLw3Ay_PfMASnpzz6f9stSfQG3F9vT\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "t815QMDUR76G",
    "outputId": "7ceff20c-fb8a-43a4-f791-3952ea754b42"
   },
   "outputs": [],
   "source": [
    "# Import functions and classes from Keras library\n",
    "from keras import Input, Sequential\n",
    "from keras.layers import LeakyReLU, Dense\n",
    "from keras.layers import Reshape, Conv2DTranspose, BatchNormalization\n",
    "\n",
    "# Specify the latent space dimension\n",
    "latent_dim = 128\n",
    "\n",
    "# Create the generator. Save it to gan_generator variable\n",
    "# Transpose convolutions should have a filter size of 5\n",
    "# The first transpose convolution has stride of 1. Others have stride of 2\n",
    "### --YOUR CODE HERE-- ###\n",
    "gan_generator = Sequential([\n",
    "    Dense(7*7*256, input_shape=(latent_dim,), name='Dense_1'),\n",
    "    BatchNormalization(name=\"Batch_Normalization\"),\n",
    "    LeakyReLU(name='Leaky_ReLu_2'),\n",
    "    Reshape((7, 7, 256),name='reshape'),\n",
    "    Conv2DTranspose(128, (5,5), strides=(1,1), padding='same', name='Conv2D_Transpose'),\n",
    "    BatchNormalization(name=\"Batch_Normalization_1\"),\n",
    "    LeakyReLU(name='Leaky_ReLu_3'),\n",
    "    Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', name='Conv2D_Transpose_1'),\n",
    "    BatchNormalization(name=\"Batch_Normalization_2\"),\n",
    "    LeakyReLU(name='Leaky_ReLu_4'),\n",
    "    Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', name='Conv2D_Transpose_2')\n",
    "])\n",
    "gan_generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPEQ-qhWSE7u"
   },
   "source": [
    "4. Execute the code below that will define a custom model for GAN and instantiate its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CW0DEieUSEYe"
   },
   "outputs": [],
   "source": [
    "# Import functions and classes from Keras library\n",
    "from keras import Model, ops\n",
    "\n",
    "# Define GAN as a custom model\n",
    "class GAN(Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_tracker = keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_tracker, self.g_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # If input is a tuple, get the first element\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = ops.shape(real_images)[0]\n",
    "        random_latent_vectors = keras.random.normal(\n",
    "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
    "        )\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = ops.concatenate([generated_images, real_images],\n",
    "                                          axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        # Use one-sided label smoothing for the \"real\" labels.\n",
    "        labels = ops.concatenate([ops.full((batch_size, 1), 0.9),\n",
    "                            ops.zeros((batch_size, 1))], axis=0)\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply(grads, self.discriminator.trainable_weights)\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = keras.random.normal(\n",
    "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = ops.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply(grads, self.generator.trainable_weights)\n",
    "\n",
    "        # Update metrics and return their value.\n",
    "        self.d_loss_tracker.update_state(d_loss)\n",
    "        self.g_loss_tracker.update_state(g_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# Instantiate the GAN model\n",
    "gan = GAN(discriminator=gan_discriminator,\n",
    "          generator=gan_generator,\n",
    "          latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MtSO11jXhsH"
   },
   "source": [
    "5. Configure the network for training with the appropriate loss function and an optimizer of your choice. Then train the model with a batch size of 100. Make sure to save the history of losses per training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWz3fKQPnB_S",
    "outputId": "5e8019ae-d9ab-42ca-adf7-dccef21482fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure the network for training\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "gan.compile(\n",
    "    d_optimizer=Adam(learning_rate=0.00005, beta_1=0.5, beta_2=0.99, epsilon=1e-7),\n",
    "    g_optimizer=Adam(learning_rate=0.0003, beta_1=0.5, beta_2=0.99, epsilon=1e-7),\n",
    "    loss_fn=BinaryCrossentropy(from_logits=True),\n",
    ") # You may add more optimizer parameters for tuning the training process\n",
    "\n",
    "# Train the model. Set the dataset batch size to 100\n",
    "# You may take 100 batches only to speed up the training\n",
    "# Assign the output to gan_hist variable\n",
    "### --YOUR CODE HERE-- ###\n",
    "gan_hist = gan.fit(\n",
    "    emnist_ds.batch(100).repeat(),\n",
    "    epochs = 50,\n",
    "    steps_per_epoch=len(emnist_train)//100,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOD8iUf4cv2E"
   },
   "source": [
    "6. Plot the generator and discriminator losses per epoch. Place appropriate plot title and axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHSolM-_cvGF"
   },
   "outputs": [],
   "source": [
    "# Extract the losses during training\n",
    "dis_losses = gan_hist.history[\"d_loss\"]\n",
    "gen_losses = gan_hist.history[\"g_loss\"]\n",
    "epochs = range(1, len(dis_losses) + 1)\n",
    "\n",
    "# Plot the history of training and validation losses\n",
    "### --YOUR CODE HERE-- ###\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Generator and Discriminator Loesses per Epoch\")\n",
    "plt.plot(epochs, dis_losses, label = \"Discriminator\")\n",
    "plt.plot(epochs, gen_losses, label = \"Generator\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMjGuTacc-J9"
   },
   "source": [
    "7. Feed the generator network with at least 30 random inputs. How many of these images resembles a valid handwritten symbol? Afterwards, add a small random offset to the initial inputs and feed it to the generator network once again. How did the output images changed? Display all output images and write your observations in the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-TC5UjtdgWO"
   },
   "outputs": [],
   "source": [
    "# Import functions and classes from Keras library\n",
    "from keras.random import normal\n",
    "\n",
    "# Generate 30 random 128-dimensional vectors as input\n",
    "# Feed the random inputs to the generator network\n",
    "# Display the output images of the generator\n",
    "### --YOUR CODE HERE-- ###\n",
    "vectors = normal(shape=(30, latent_dim))\n",
    "#image generation step\n",
    "generated = gan_generator(vectors).numpy()\n",
    "\n",
    "#plotting\n",
    "plt.figure(figsize=(15,5))\n",
    "for i in range(30):\n",
    "  plt.subplot(3,10,i+1)\n",
    "  plt.imshow(generated[i], cmap=\"gray\")\n",
    "  plt.axis(\"off\")\n",
    "plt.suptitle(\"Generated Images\")\n",
    "plt.show()\n",
    "\n",
    "# Add a small random offsets to the initial inputs\n",
    "# Feed the new input to the generator network\n",
    "# Display the new output images of the generator\n",
    "### --YOUR CODE HERE-- ###\n",
    "noise = np.random.normal(0, 0.1, size = vectors.shape)\n",
    "vectors_offset = vectors + noise\n",
    "generated_offset = gan_generator(vectors_offset).numpy()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for i in range(30):\n",
    "  plt.subplot(3,10,i+1)\n",
    "  plt.imshow(generated_offset[i], cmap=\"gray\")\n",
    "  plt.axis(\"off\")\n",
    "plt.suptitle(\"Generated Images with Offset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDXITKpu9nGJ"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dIY4kvdVTW8"
   },
   "source": [
    "### Machine Problems\n",
    "\n",
    "1. Generative adversarial networks generate images based purely on random noise, which means the user has little control over what type of image is generated. On the other hand, Conditional GANs (CGAN) [7] allow you to condition the image generation process on additional information, such as class labels or other data features. Train a CGAN with the EMNIST dataset so that the generated character can be controlled.  You may refer to these tutorials as a guide: [1](https://keras.io/examples/generative/conditional_gan/), [2](https://www.kaggle.com/code/arturlacerda/pytorch-conditional-gan/notebook).\n",
    "\n",
    "    Show the plot of discriminator and generator losses per epoch during training. Perform hyperparameter tuning of the networks and document the results. Generate 5 images each from at least 10 different characters.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/0*SXRW2IxUeDxpPj7j.png\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Preprocessing MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        csv_path = \"/home/pj/Documents/Academics/LBYCPC4/archive/fashion-mnist_train.csv\"\n",
    "        fashion_df = pd.read_csv(csv_path)\n",
    "        self.labels = fashion_df.label.values\n",
    "        self.images = fashion_df.iloc[:, 1:].values.astype('uint8').reshape(-1, 28, 28)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = Image.fromarray(self.images[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FashionMNIST()\n",
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "dataset = FashionMNIST(transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(794, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        x = x.view(x.size(0), 784)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([x, c], 1)\n",
    "        out = self.model(x)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(110, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        z = z.view(z.size(0), 100)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        out = self.model(x)\n",
    "        return out.view(x.size(0), 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().cuda()\n",
    "discriminator = Discriminator().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n",
    "    g_optimizer.zero_grad()\n",
    "    z = Variable(torch.randn(batch_size, 100)).cuda()\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).cuda()\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    validity = discriminator(fake_images, fake_labels)\n",
    "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).cuda())\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return g_loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    # train with real images\n",
    "    real_validity = discriminator(real_images, labels)\n",
    "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).cuda())\n",
    "    \n",
    "    # train with fake images\n",
    "    z = Variable(torch.randn(batch_size, 100)).cuda()\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).cuda()\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    fake_validity = discriminator(fake_images, fake_labels)\n",
    "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).cuda())\n",
    "    \n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    return d_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "n_critic = 5\n",
    "display_step = 300\n",
    "\n",
    "# store losses\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}...'.format(epoch))\n",
    "\n",
    "    epoch_g_loss = 0.0\n",
    "    epoch_d_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        real_images = Variable(images).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        generator.train()\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        d_loss = discriminator_train_step(len(real_images), discriminator,\n",
    "                                          generator, d_optimizer, criterion,\n",
    "                                          real_images, labels)\n",
    "        \n",
    "\n",
    "        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
    "\n",
    "        # accumulate batch losses\n",
    "        epoch_d_loss += d_loss\n",
    "        epoch_g_loss += g_loss\n",
    "        num_batches += 1\n",
    "\n",
    "        generator.eval()\n",
    "\n",
    "    # average loss per epoch\n",
    "    avg_d_loss = epoch_d_loss / num_batches\n",
    "    avg_g_loss = epoch_g_loss / num_batches\n",
    "\n",
    "    d_losses.append(avg_d_loss)\n",
    "    g_losses.append(avg_g_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | g_loss: {avg_g_loss:.4f}, d_loss: {avg_d_loss:.4f}\")\n",
    "\n",
    "    # sample images for visualization\n",
    "    z = Variable(torch.randn(9, 100)).cuda()\n",
    "    labels = Variable(torch.LongTensor(np.arange(9))).cuda()\n",
    "    sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n",
    "    grid = make_grid(sample_images, nrow=3, normalize=True).permute(1,2,0).numpy()\n",
    "    plt.imshow(grid)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of Discriminator and Generator Losses Per Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs+1), g_losses, label=\"Generator Loss\")\n",
    "plt.plot(range(1, num_epochs+1), d_losses, label=\"Discriminator Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Generator and Discriminator Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(torch.randn(100, 100)).cuda()\n",
    "labels = Variable(torch.LongTensor([i for _ in range(10) for i in range(10)])).cuda()\n",
    "sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n",
    "grid = make_grid(sample_images, nrow=10, normalize=True).permute(1,2,0).numpy()\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.imshow(grid)\n",
    "_ = plt.yticks([])\n",
    "_ = plt.xticks(np.arange(15, 300, 30), ['T-Shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], rotation=45, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtS1UxTpVZm8"
   },
   "source": [
    "### References\n",
    "\n",
    "[1] Aggarwal, Charu C. (2023). *Neural Networks and Deep Learning : A Textbook*. 2nd ed. Cham: Springer International Publishing.\n",
    "\n",
    "[2] Chollet, F. et al., (2025). *Keras 3 API documentation*. Retrieved from https://keras.io/api/\n",
    "\n",
    "[3] Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). *EMNIST: an extension of MNIST to handwritten letters*. Retrieved from http://arxiv.org/abs/1702.05373\n",
    "\n",
    "[4] Kingma, D., & Welling, M. (2013). *Auto-Encoding Variational Bayes*. Retrieved from https://arxiv.org/abs/1312.6114\n",
    "\n",
    "[5] Goodfellow, I., Bengio, Y., & Courville, A. (2016) *Deep Learning*. MIT Press.\n",
    "\n",
    "[6] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). *Generative Adversarial Networks*. Retrieved from https://arxiv.org/abs/1406.2661\n",
    "\n",
    "[7] Mirza, M., & Osindero, S. (2014). *Conditional Generative Adversarial Nets*. Retrieved from https://arxiv.org/abs/1411.1784v1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ML-GPU)",
   "language": "python",
   "name": "ml-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
