{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi7tm-bw_k9z"
      },
      "source": [
        "**Denoising Autoencoder**\n",
        "*   Philip Jeremiah Caleon\n",
        "*   12013544"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99cvQHcK_-cE"
      },
      "source": [
        "Download this link:\n",
        "[Image](https://drive.google.com/file/d/1TLYDxKhdD-DJhQ805iIfs6V_e8vZblwU/view)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcdw4htJANON"
      },
      "source": [
        "# Extracting Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IDvSz6KA1o7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to your PNG file\n",
        "img_path = \"/home/pj/Documents/Academics/CPECOG3/noisy_image3.png\"\n",
        "\n",
        "# Open the image\n",
        "img = Image.open(img_path)\n",
        "\n",
        "# Display with matplotlib\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9QCzvFlAV1L"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnlVeE5fV9aW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5cBj7AhAXv1"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K95FSBiV9aX"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10\n",
        "(x_train, _), (x_test, _) = cifar10.load_data()\n",
        "\n",
        "# Select subset\n",
        "x_subset = x_train[:5000]\n",
        "\n",
        "# Normalize to 0–1\n",
        "x_subset = x_subset.astype(\"float32\") / 255.0\n",
        "x_test   = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Add synthetic noise to the images\n",
        "noise_factor = 0.2\n",
        "x_subset_noisy = x_subset + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_subset.shape)\n",
        "x_test_noisy   = x_test   + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "# Clip values to stay in [0,1]\n",
        "x_subset_noisy = np.clip(x_subset_noisy, 0., 1.)\n",
        "x_test_noisy   = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "# Show a few noisy vs clean pairs\n",
        "plt.figure(figsize=(10, 2))\n",
        "for i in range(5):\n",
        "    # Noisy image\n",
        "    ax = plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(x_subset_noisy[i])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Clean image\n",
        "    ax = plt.subplot(2, 5, i+1+5)\n",
        "    plt.imshow(x_subset[i])\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQFTYPJYAa_Z"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH1U2CmdV9aY"
      },
      "outputs": [],
      "source": [
        "# input layer\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, Input, Dense, Reshape, Conv2DTranspose,\\\n",
        "   Activation, BatchNormalization, ReLU, Concatenate\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Create the autoencoder model based from the illustrated model plot\n",
        "# Save the model as autoencoder_cnn variable\n",
        "### --YOUR CODE HERE-- ###\n",
        "input_img = layers.Input(shape=(32,32,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u24wGM8lV9aY"
      },
      "outputs": [],
      "source": [
        "## Encoder\n",
        "x1 = Conv2D(32, (3,3), strides=2, padding='same')(input_img)  # downsample\n",
        "x1 = BatchNormalization()(x1)\n",
        "x1 = ReLU()(x1)\n",
        "\n",
        "x2 = Conv2D(64, (3,3), strides=2, padding='same')(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "x2 = ReLU()(x2)\n",
        "\n",
        "x3 = Conv2D(128, (3,3), strides=2, padding='same')(x2)\n",
        "x3 = BatchNormalization()(x3)\n",
        "x3 = ReLU()(x3)\n",
        "\n",
        "x4 = Conv2D(256, (3,3), strides=2, padding='same')(x3)\n",
        "x4 = BatchNormalization()(x4)\n",
        "x4 = ReLU()(x4)\n",
        "\n",
        "# Bottleneck\n",
        "bottleneck = Conv2D(256, (3,3), padding='same')(x4)\n",
        "bottleneck = BatchNormalization()(bottleneck)\n",
        "bottleneck = ReLU()(bottleneck)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3vm_xX1V9aY"
      },
      "outputs": [],
      "source": [
        "## Decoder (mirroring encoder with skip connections)\n",
        "d1 = Conv2DTranspose(256, (3,3), strides=2, padding='same')(bottleneck)\n",
        "d1 = BatchNormalization()(d1)\n",
        "d1 = ReLU()(d1)\n",
        "d1 = Concatenate()([d1, x3])   # skip connection\n",
        "\n",
        "d2 = Conv2DTranspose(128, (3,3), strides=2, padding='same')(d1)\n",
        "d2 = BatchNormalization()(d2)\n",
        "d2 = ReLU()(d2)\n",
        "d2 = Concatenate()([d2, x2])\n",
        "\n",
        "d3 = Conv2DTranspose(64, (3,3), strides=2, padding='same')(d2)\n",
        "d3 = BatchNormalization()(d3)\n",
        "d3 = ReLU()(d3)\n",
        "d3 = Concatenate()([d3, x1])\n",
        "\n",
        "d4 = Conv2DTranspose(32, (3,3), strides=2, padding='same')(d3)\n",
        "d4 = BatchNormalization()(d4)\n",
        "d4 = ReLU()(d4)\n",
        "\n",
        "# Output\n",
        "decoded = Conv2DTranspose(3, (3,3), padding='same', activation='sigmoid')(d4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPfgxARYV9aY"
      },
      "outputs": [],
      "source": [
        "autoencoder_cnn = models.Model(input_img, decoded)\n",
        "autoencoder_cnn.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "autoencoder_cnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spNqboXJAhBs"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1eDjxpLV9aZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "hist_autoencoder_cnn = autoencoder_cnn.fit(\n",
        "    x_subset_noisy, x_subset,   # train noisy → train clean\n",
        "    epochs=5,\n",
        "    batch_size=100,\n",
        "    validation_data=(x_test_noisy, x_test),  # test noisy → test clean\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMghqrj7AiMB"
      },
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Load and preprocess the noisy image\n",
        "img = Image.open(img_path).convert(\"RGB\")   # ensure RGB\n",
        "img = img.resize((32, 32))                  # resize to CIFAR-10 size\n",
        "img_array = img_to_array(img).astype(\"float32\") / 255.0\n",
        "img_array = np.expand_dims(img_array, axis=0)  # add batch dimension\n",
        "\n",
        "# Run the image through the trained autoencoder\n",
        "denoised_img = autoencoder_img.predict(img_array)\n",
        "\n",
        "# Remove batch dimension for display\n",
        "denoised_img = np.squeeze(denoised_img, axis=0)\n",
        "\n",
        "# Plot original vs denoised\n",
        "plt.figure(figsize=(6,3))\n",
        "\n",
        "# Original noisy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_array[0])\n",
        "plt.title(\"Noisy Input\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Denoised output\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(denoised_img)\n",
        "plt.title(\"Denoised Output\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NtqF3dcnXWIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (ML-GPU)",
      "language": "python",
      "name": "ml-gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}